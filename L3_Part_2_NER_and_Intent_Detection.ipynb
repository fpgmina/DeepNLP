{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/L3_Part_2_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Giuseppe Gallipoli\n",
        "\n",
        "**Credits:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entity Recognition (part 1) & Intent Detection (part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrJbaVgi_ja"
      },
      "source": [
        "## Intent Detection\n",
        "\n",
        "In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of his/her behavior in interaction with a computer system, such as in search engines. Intent Detection is the identification and categorization of what a user online intended or wanted to find when they type or speak with a conversational agent (or a search engine).\n",
        "\n",
        "![https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png](https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png)\n",
        "\n",
        "In this section, you will use the ATIS dataset: https://github.com/yvchen/JointSLU; https://www.kaggle.com/siddhadev/atis-dataset-clean/home\n",
        "\n",
        "The task is to classify the intent of a sentence. The dataset is split into train, validation and test sets. **Use the provided splits** to train and evaluate your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L6-2ABir0yS"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nJxxjTj8Q9"
      },
      "source": [
        "### **Question 5: Two-step classification model**\n",
        "\n",
        "Train a classification model to identify the intent from a given sentence. The model is required to leverage on pre-trained BERT model to generate sentence embeddings (important: **no fine-tuning**). The model is required to use the embeddings to perform classification.\n",
        "\n",
        "Once extracted the embeddings, you can use any classifier you want. For example, you can use a linear classifier (e.g., Logistic Regression) or a neural network (e.g., MLP). For your convenience, you can use the `sklearn` library for training the classifier (https://scikit-learn.org/stable/supervised_learning.html).\n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true)\n",
        "\n",
        "\n",
        "Assess the performance of the trained model (the model on top of BERT) on the test set by using the **classification accuracy**, **precision**, **recall** and **F1-score**. You can use the `sklearn` library for computing the metrics (https://scikit-learn.org/stable/api/sklearn.metrics.html).\n",
        "\n",
        "\n",
        "Note: you can use the `sentence-transformers` library to generate sentence embeddings (https://www.sbert.net/docs/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKxa9aEsEZSk"
      },
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "ZQBBUDMO6xYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xrNtQTq5WW"
      },
      "source": [
        "### **Question 6: Fine-tuning end-to-end classification model**\n",
        "\n",
        "Another approach is to fine-tune the BERT model for the classification task. A classification head is added on top of the pre-trained BERT model. The classification head is trained end-to-end with the BERT model.\n",
        "This approach is more effective than the previous one because the model is trained end-to-end. However, the model requires more training time and resources.\n",
        "\n",
        "Train a new BERT model for the task of [sequence classification](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification) (include BERT fine-tuning).  \n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true)\n",
        "\n",
        "Assess the performance of the generated model by using the same metrics used in the previous question.\n",
        "\n",
        "Which model has better performance? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlKATw5sJAY"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7: Intent Detection using Large Language Models**\n",
        "**Credits:** Giuseppe Gallipoli\n",
        "\n",
        "#### Introduction\n",
        "[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) are a type of deep learning model capable of language generation. These models are built on deep learning architectures, primarily using neural networks, and are trained on massive amounts of text data. LLMs generally leverage the *Transformer* architecture, which allows them to process language in context, capturing complex relationships between words and concepts.\n",
        "\n",
        "Large Language Models have demonstrated excellent capabilities across a wide variety of tasks, making them versatile models which can be applied in diverse scenarios and use cases. Although they are more typically used for *generative* tasks (e.g., text generation, text summarization, open-ended Question Answering), they can also be employed in *discriminative* tasks.\n",
        "\n",
        "In this practice, we will use a Large Language Model to address an intent detection task. Rather than using a pre-trained encoder-only model without (Question 5) or with fine-tuning (Question 6), we will ask the LLM to classify the intent given a sentence of interest.\n",
        "<br><br>\n",
        "For now, we will use the [Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) 3.8B model, i.e., `microsoft/Phi-4-mini-instruct`."
      ],
      "metadata": {
        "id": "QgxMwtDiePfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM Prompting\n",
        "To interact with LLMs, we need to define a **prompt**, which is a piece of text containing the instruction or question we want to give or ask the model. As we saw in Practice 1 (Exercise 11), a prompt can both include only the instruction/question for the LLM but also additional information (i.e., *context*) which can be exploited by the model to generate the answer.\n",
        "\n",
        "For now, we will ask the model to classify the intent of a given sentence <u>without providing</u> any additional context. Please note that, since we want to (potentially) limit the choice of the LLM to a predefined list of intents (i.e., the set of labels of our dataset), we will also provide this list to the model.\\\n",
        "*Example of prompt*:\\\n",
        "Which is the intent of the following sentence?\\\n",
        "Choose among: label1, label2, ...\n",
        "<br><br>\n",
        "\n",
        "**Hint**: For those data instances whose intent is the concatenation of two intents (i.e., `atis_flight#atis_airfare`), consider only the first one.\n",
        "\n",
        "<u>Suggestion</u>: To increase speed, switch to a GPU runtime. You can do this by clicking on Runtime → Change runtime type → Hardware accelerator → Select T4 GPU.\\\n",
        "If you encounter an `OutOfMemoryError`, try restarting the session by clicking on Runtime → Restart session."
      ],
      "metadata": {
        "id": "yc1Nz6DueRm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "MnMuuWo-eUqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few-Shot Learning\n",
        "When we provide the LLM with additional context to be leveraged for generating an answer, this is known as *in-context learning* (ICL).\\\n",
        "A common technique to perform ICL involves including one or more **additional examples** of questions (or instructions) and their expected answers in the prompt.\\\n",
        "This can be useful for several reasons: telling the model the required output format, both in terms of structure and style, making the model reason about the provided examples to better adapt to the (new) task, tailoring model's responses to a user's specific needs, ...\\\n",
        "All of this is done directly at inference, without the need of further training the model, simply by leveraging the reasoning and generalization capabilities of LLMs.\\\n",
        "When no examples are provided, as in the previous point of the exercise, we talk about **zero-shot** learning. Conversely, when we supply additional input-output examples in the model's prompt, we talk about **few-shot** learning or $k$-shot learning, where $k$ represents the number of ICL examples.\\\n",
        "Examples can be selected according to different strategies, either randomly or in a more clever way, but please note that they <u>must be chosen from the training set</u> to avoid *data leakage*.\n",
        "<br><br>\n",
        "\n",
        "Now, try to implement few-shot learning by modifying the previous point of the exercise so that the prompt can include $k$ additional examples."
      ],
      "metadata": {
        "id": "r4HtEnZpeXSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "dCDzc5bqeZ9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different $k$ values, e.g., $k \\in [1, 3, 5, 10]$, and see if and how the model's performance changes."
      ],
      "metadata": {
        "id": "UTozppkMeaT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "ogVHq4AkeanX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now, we have used the [Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) 3.8B model, i.e., `microsoft/Phi-4-mini-instruct`, but there are plenty of LLMs!\\\n",
        "Try another model of your choice, e.g., [Mistral-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) 7B `mistralai/Mistral-7B-Instruct-v0.3`. Remember to always keep in mind the GPU memory constraints you have.\n",
        "<br><br>\n",
        "\n",
        "**Hint**: To download and use certain models, it may be needed to request access to them (e.g., [Llama3.2-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) 3B). This can be done from the Hugging Face model page after logging in. Once access is granted, you need to create your personal access token (go to your HF profile, Settings → Access Tokens → Create new token) and authenticate using it when downloading both the model and the tokenizer.\\\n",
        "To authenticate, you can either use the additional `token` parameter in the `from_pretrained()` method or via the [HF Command Line Interface](https://huggingface.co/docs/huggingface_hub/guides/cli).\n",
        "```\n",
        "model_tag = 'MODEL_TAG'\n",
        "HF_TOKEN = 'MY_HF_TOKEN'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_tag, torch_dtype=torch.float16, token=HF_TOKEN, device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_tag, token=HF_TOKEN)\n",
        "```"
      ],
      "metadata": {
        "id": "ZivPCYgxed6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "zElQT4t4ef5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing the performance of the different settings and/or models, here you can find some questions to reason about the results:\n",
        "- What is the performance in the zero-shot learning setting? (i.e., $k=0$)\n",
        "- How does it change when providing one additional example? (i.e., $k=1$)\n",
        "- What happens when increasing the number of ICL examples?\n",
        "- If you tested additional models, which one performs best in the zero-shot and few-shot learning settings?\n",
        "- What challenges or limitations did you observe?"
      ],
      "metadata": {
        "id": "qcjsfn4Jehap"
      }
    }
  ]
}