{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/L3_Part_2_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Giuseppe Gallipoli\n",
        "\n",
        "**Credits:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entity Recognition (part 1) & Intent Detection (part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrJbaVgi_ja"
      },
      "source": [
        "## Intent Detection\n",
        "\n",
        "In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of his/her behavior in interaction with a computer system, such as in search engines. Intent Detection is the identification and categorization of what a user online intended or wanted to find when they type or speak with a conversational agent (or a search engine).\n",
        "\n",
        "![https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png](https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png)\n",
        "\n",
        "In this section, you will use the ATIS dataset: https://github.com/yvchen/JointSLU; https://www.kaggle.com/siddhadev/atis-dataset-clean/home\n",
        "\n",
        "The task is to classify the intent of a sentence. The dataset is split into train, validation and test sets. **Use the provided splits** to train and evaluate your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L6-2ABir0yS"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nJxxjTj8Q9"
      },
      "source": [
        "### **Question 5: Two-step classification model**\n",
        "\n",
        "Train a classification model to identify the intent from a given sentence. The model is required to leverage on pre-trained BERT model to generate sentence embeddings (important: **no fine-tuning**). The model is required to use the embeddings to perform classification.\n",
        "\n",
        "Once extracted the embeddings, you can use any classifier you want. For example, you can use a linear classifier (e.g., Logistic Regression) or a neural network (e.g., MLP). For your convenience, you can use the `sklearn` library for training the classifier (https://scikit-learn.org/stable/supervised_learning.html).\n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true)\n",
        "\n",
        "\n",
        "Assess the performance of the trained model (the model on top of BERT) on the test set by using the **classification accuracy**, **precision**, **recall** and **F1-score**. You can use the `sklearn` library for computing the metrics (https://scikit-learn.org/stable/api/sklearn.metrics.html).\n",
        "\n",
        "\n",
        "Note: you can use the `sentence-transformers` library to generate sentence embeddings (https://www.sbert.net/docs/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKxa9aEsEZSk"
      },
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "TEXT_COLUMN = 'tokens'\n",
        "LABEL_COLUMN = 'intent'\n",
        "\n",
        "try:\n",
        "  # Load training and test data using pandas\n",
        "  train_df = pd.read_csv(\"atis.train.csv\")\n",
        "  test_df = pd.read_csv(\"atis.test.csv\")\n",
        "  # We also load the dev (validation) set, though we won't use it in this\n",
        "  # specific train/test script.\n",
        "  dev_df = pd.read_csv(\"atis.dev.csv\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\"=\"*50)\n",
        "  print(\"ERROR: Could not find data files.\")\n",
        "  print(\"Please make sure 'atis.train.csv', 'atis.dev.csv', and 'atis.test.csv' \")\n",
        "  print(\"are in the same directory as this script.\")\n",
        "  print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ZQBBUDMO6xYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "nOXq2e4O1bdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = train_df[TEXT_COLUMN].tolist()\n",
        "y_train = train_df[LABEL_COLUMN].tolist()\n",
        "\n",
        "sentences_test = test_df[TEXT_COLUMN].tolist()\n",
        "y_test = test_df[LABEL_COLUMN].tolist()\n",
        "\n",
        "num_train = len(sentences_train)\n",
        "num_test = len(sentences_test)\n",
        "num_classes = len(set(y_train)) # Get the number of unique intents\n",
        "\n",
        "print(f\"Loaded {num_train} training sentences from 'atis.train.csv'.\")\n",
        "print(f\"Loaded {num_test} test sentences from 'atis.test.csv'.\")\n",
        "print(f\"Found {num_classes} unique intent classes in the training data.\")\n",
        "print(\"-\" * 30, \"\\n\")"
      ],
      "metadata": {
        "id": "uKtvs6bE1qi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(y_train)"
      ],
      "metadata": {
        "id": "_4IINce218_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Sentence Embeddings (no-fine-tuning)"
      ],
      "metadata": {
        "id": "cNbrtOin3ZEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on all-MiniLM-L6-v2:\n",
        "\n",
        "1. What is all-MiniLM-L6-v2?\n",
        "\n",
        "It is a Transformer model, BERT based. The \"LM\" stands for \"Language Model.\" It was created by Microsoft using a process called \"distillation\" from a much larger model. Think of it as a small, fast, and highly optimized version of BERT.\n",
        "\n",
        "SentenceTransformer Model: This specific model has been further trained by the SentenceTransformer library (using a siamese network structure) specifically for the task of creating high-quality sentence embeddings.\n",
        "\n",
        "2. Why Not Use bert-base-uncased Directly?\n",
        "\n",
        "This is the most important part, and it relates directly to your \"no fine-tuning\" constraint.\n",
        "\n",
        "A standard BERT model (like bert-base-uncased from the transformers library) is a great base for fine-tuning.\n",
        "\n",
        "However, if you just take its raw output embeddings without any fine-tuning (for example, by taking the [CLS] token's embedding), they are generally not very good for representing the meaning of a whole sentence. The model's pre-training (Masked Language Model) doesn't teach it to create good sentence-level representations by default.\n",
        "\n",
        "You would get embeddings, but they would likely lead to poor classification accuracy."
      ],
      "metadata": {
        "id": "bjMObzx4305i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# Convert sentences to vector representations (embeddings)\n",
        "print(\"Encoding training sentences... \")\n",
        "X_train_embeddings = embedder.encode(sentences_train, show_progress_bar=True)\n",
        "print(\"Encoding test sentences...\")\n",
        "X_test_embeddings = embedder.encode(sentences_test, show_progress_bar=True)\n",
        "\n",
        "print(f\"Embeddings generated. Shape of training embeddings: {X_train_embeddings.shape}\")\n",
        "print(f\"Embeddings generated. Shape of test embeddings: {X_test_embeddings.shape}\")\n",
        "print(\"-\" * 30, \"\\n\")"
      ],
      "metadata": {
        "id": "PKIA9_F23YvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification"
      ],
      "metadata": {
        "id": "TcGtjzTv4o7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A note on sklearn LogisticRegression:\n",
        "\n",
        "While its name comes from logistic (binary) regression, the scikit-learn implementation of LogisticRegression handles multi-class classification problems automatically.\n",
        "\n",
        "It does this using one of two main strategies:\n",
        "\n",
        "1. One-vs-Rest (OvR) (Default): This is the default method. If you have N classes (e.g., \"greeting\", \"goodbye\", \"order_pizza\"), it trains N separate binary logistic regression classifiers:\n",
        "\n",
        "    * One classifier to distinguish \"greeting\" (class 1) from \"all other classes\" (class 0).\n",
        "\n",
        "    * One classifier to distinguish \"goodbye\" (class 1) from \"all other classes\" (class 0).\n",
        "\n",
        "    * One classifier to distinguish \"order_pizza\" (class 1) from \"all other classes\" (class 0). When you give it a new sentence, it runs all N classifiers and picks the class that gives the highest confidence score.\n",
        "\n",
        "2. Multinomial (Softmax): You can also explicitly tell it to use this method by setting multi_class='multinomial'. This trains a single classifier model that directly outputs a probability for every class, all summing to 1. This is often called Softmax Regression."
      ],
      "metadata": {
        "id": "FPlXAjMa5lBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training Classifier"
      ],
      "metadata": {
        "id": "bazPn8Bb47TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# We increase max_iter as the dataset is larger and may need more iterations to converge.\n",
        "classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "print(\"Training classifier...\")\n",
        "# Train the classifier on the *training* embeddings and labels\n",
        "classifier.fit(X_train_embeddings, y_train)\n",
        "\n",
        "print(\"Classifier training complete.\")\n",
        "print(\"-\" * 30, \"\\n\")"
      ],
      "metadata": {
        "id": "EmkuTrfv4svO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Eval Classifier"
      ],
      "metadata": {
        "id": "xNwafc2V4_pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test_embeddings)\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "\n",
        "# We use average='weighted' to account for class imbalance (some intents\n",
        "# are more common than others).\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\n--- Overall Metrics (Weighted Average) ---\")\n",
        "print(f\"**Accuracy:** {accuracy:.4f}\")\n",
        "print(f\"**Precision:** {precision:.4f}\")\n",
        "print(f\"**Recall:** {recall:.4f}\")\n",
        "print(f\"**F1-Score:** {f1:.4f}\")\n",
        "\n",
        "# --- Detailed Classification Report ---\n",
        "# This is now much more useful as it shows performance per-intent\n",
        "print(\"\\n--- Detailed Classification Report (Per-Intent) ---\")\n",
        "# We set `labels` to the unique classes found in the test set to ensure\n",
        "# the report is complete and in a consistent order.\n",
        "unique_labels = sorted(list(set(y_test)))\n",
        "report = classification_report(y_test, y_pred, labels=unique_labels, zero_division=0)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "oqgCfXVq5JQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xrNtQTq5WW"
      },
      "source": [
        "### **Question 6: Fine-tuning end-to-end classification model**\n",
        "\n",
        "Another approach is to fine-tune the BERT model for the classification task. A classification head is added on top of the pre-trained BERT model. The classification head is trained end-to-end with the BERT model.\n",
        "This approach is more effective than the previous one because the model is trained end-to-end. However, the model requires more training time and resources.\n",
        "\n",
        "Train a new BERT model for the task of [sequence classification](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification) (include BERT fine-tuning).  \n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true)\n",
        "\n",
        "Assess the performance of the generated model by using the same metrics used in the previous question.\n",
        "\n",
        "Which model has better performance? Why?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "dIR4XLG_MdJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Labels"
      ],
      "metadata": {
        "id": "69ZDMfEtLvue"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlKATw5sJAY"
      },
      "source": [
        "# Transformers models require integer labels.\n",
        "# We create a mapping from the string intent (e.g., 'atis_flight') to an integer (e.g., 0).\n",
        "unique_labels = sorted(list(train_df['intent'].unique()))\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id_to_label = {i: label for i, label in enumerate(unique_labels)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id"
      ],
      "metadata": {
        "id": "1mek96NBMJX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the mapping to our dataframes\n",
        "train_df['label'] = train_df['intent'].map(label_to_id)\n",
        "dev_df['label'] = dev_df['intent'].map(label_to_id)\n",
        "test_df['label'] = test_df['intent'].map(label_to_id)\n",
        "\n",
        "# --- Convert to Hugging Face Dataset object ---\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "dev_dataset = Dataset.from_pandas(dev_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "NmnmZuC9MWXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Loading and Tokenization"
      ],
      "metadata": {
        "id": "UbymScRsNVwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use DistilBERT. It's a smaller, faster version of BERT that\n",
        "# retains ~97% of its performance. It's perfect for Colab.\n",
        "# The principle is IDENTICAL to using 'bert-base-uncased'.\n",
        "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Create a function to tokenize the 'sentence' column\n",
        "def tokenize_function(examples):\n",
        "    # padding=\"max_length\" pads all sentences to the same length.\n",
        "    # truncation=True cuts off sentences that are too long.\n",
        "    return tokenizer(examples['tokens'], padding='max_length', truncation=True)\n",
        "\n",
        "# Apply the tokenization to all datasets at once using .map()\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "-EMpXoxaNZVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoModelForSequenceClassification`\n",
        "\n",
        "* Auto: This is the \"automatic\" part. You give it a model checkpoint name (like \"bert-base-uncased\" or \"distilbert-base-uncased\"), and it automatically knows which model architecture to load (BERT, DistilBERT, RoBERTa, etc.). You don't have to manually import BertForSequenceClassification or DistilBertForSequenceClassification.\n",
        "\n",
        "* Model: It loads a pre-trained Transformer model (like BERT), which acts as the \"body.\" This part is already an expert at understanding language.\n",
        "\n",
        "* ForSequenceClassification: This is the key part. It tells the Auto class to add a classification \"head\" on top of the model's \"body.\" \"Sequence Classification\" is the task of taking one sequence (like a sentence) and assigning it a single label (like \"positive,\" \"negative,\" or \"atis_flight\")."
      ],
      "metadata": {
        "id": "9LEfBLTENs2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "# AutoModelForSequenceClassification automatically does this:\n",
        "# 1. Loads the pre-trained BERT/DistilBERT body.\n",
        "# 2. ADDS A CLASSIFICATION HEAD on top.\n",
        "# 3. Knows it needs to be fine-tuned.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=len(unique_labels),\n",
        "    id2label=id_to_label, # For cleaner outputs\n",
        "    label2id=label_to_id\n",
        ")"
      ],
      "metadata": {
        "id": "x3mdSSHnN5x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "H8CMK1AsOiU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import EvalPrediction\n",
        "from typing import Dict\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes classification metrics for the model's predictions.\n",
        "\n",
        "    This function is passed to the `Trainer` and is called at the end of\n",
        "    each evaluation phase.\n",
        "\n",
        "    Args:\n",
        "        eval_pred (EvalPrediction): A named tuple provided by the Trainer,\n",
        "            containing:\n",
        "            - predictions (np.ndarray): The raw logits (unnormalized scores)\n",
        "              output by the model. Shape is (num_samples, num_labels).\n",
        "            - label_ids (np.ndarray): The true labels for the evaluation set.\n",
        "              Shape is (num_samples,).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary mapping metric names (e.g., 'accuracy', 'f1')\n",
        "                          to their float values. This dictionary is logged by\n",
        "                          the Trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get Logits and Labels\n",
        "    # 'eval_pred.predictions' holds the raw logits from the model's final layer.\n",
        "    # 'eval_pred.label_ids' holds the ground-truth labels.\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "\n",
        "    # 2. Convert Logits to Class Predictions\n",
        "    # We take the 'argmax' of the logits along the last dimension (axis=-1)\n",
        "    # to find the index (i.e., the class ID) with the highest score.\n",
        "    # This is our model's final \"guess\" for each input.\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # 3. Calculate Scikit-learn Metrics\n",
        "\n",
        "    # We use 'average=\"weighted\"' because this is a multi-class\n",
        "    # problem. This calculates metrics for each class independently and then\n",
        "    # computes a weighted average based on the number of samples in each\n",
        "    # class (its \"support\"). This is crucial for imbalanced datasets.\n",
        "    # 'zero_division=0' prevents warnings if a class has no predictions.\n",
        "    precision = precision_score(y_true=labels, y_pred=predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true=labels, y_pred=predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true=labels, y_pred=predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    # accuracy_score calculates the simple, overall accuracy.\n",
        "    # (Total correct predictions / Total predictions)\n",
        "    acc = accuracy_score(y_true=labels, y_pred=predictions)\n",
        "\n",
        "    # 4. Return Metrics as a Dictionary\n",
        "    # The Trainer expects a dictionary where keys are the metric names.\n",
        "    # These names will be used in the logging (e.g., \"eval_f1\").\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "T35Oh7g0OEJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These tell the Trainer how to train the model\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"atis_finetune_results\",  # Where to save the model\n",
        "  num_train_epochs=3,                 # 3 epochs is standard for fine-tuning\n",
        "  learning_rate=2e-5,                 # Standard learning rate for BERT\n",
        "  per_device_train_batch_size=16,\n",
        "  per_device_eval_batch_size=16,\n",
        "  weight_decay=0.01,\n",
        "  eval_strategy=\"epoch\",              # Run evaluation at the end of each epoch\n",
        "  save_strategy=\"epoch\",              # Save the model at the end of each epoch\n",
        "  logging_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,        # Load the best model (by loss) at the end\n",
        "  report_to=\"none\"                    # Disables extra logging (like wandb)\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=tokenized_train_dataset,\n",
        "  eval_dataset=tokenized_dev_dataset,  # Use dev set for validation\n",
        "  compute_metrics=compute_metrics,\n",
        "  processing_class=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "BPhnfH3yOp5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, trainer.train() will update all the model weights—both the pre-trained BERT/DistilBERT body and the new classification head.\n",
        "\n",
        "This is the very definition of fine-tuning.\n",
        "\n",
        "When the model is loaded, all its parameters are \"unfrozen\" (i.e., they are set to requires_grad=True). When the Trainer computes the loss from your ATIS data, the error gradient flows all the way back through the entire network.\n",
        "\n",
        "This means:\n",
        "\n",
        "* The Classification Head (which started with random weights) learns to map the BERT outputs to your specific intents.\n",
        "\n",
        "* The BERT Body (which started with general language knowledge) has its weights slightly adjusted to get better at producing representations that are specifically useful for the ATIS intent task."
      ],
      "metadata": {
        "id": "uI9VAIKhPDT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "dGXujptFPHb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval"
      ],
      "metadata": {
        "id": "bqXek9tOQm4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'trainer' object now holds the optimized model from training.\n",
        "# We can directly evaluate it on the unseen test set.\n",
        "\n",
        "print(\"Running final evaluation on the test set...\")\n",
        "eval_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
        "\n",
        "print(\"\\n--- Test Set Performance ---\")\n",
        "print(f\"**Accuracy:** {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"**F1-Score (Weighted):** {eval_results['eval_f1']:.4f}\")\n",
        "print(f\"**Precision (Weighted):** {eval_results['eval_precision']:.4f}\")\n",
        "print(f\"**Recall (Weighted):** {eval_results['eval_recall']:.4f}\")"
      ],
      "metadata": {
        "id": "6OU1ukqdQpVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7: Intent Detection using Large Language Models**\n",
        "**Credits:** Giuseppe Gallipoli\n",
        "\n",
        "#### Introduction\n",
        "[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) are a type of deep learning model capable of language generation. These models are built on deep learning architectures, primarily using neural networks, and are trained on massive amounts of text data. LLMs generally leverage the *Transformer* architecture, which allows them to process language in context, capturing complex relationships between words and concepts.\n",
        "\n",
        "Large Language Models have demonstrated excellent capabilities across a wide variety of tasks, making them versatile models which can be applied in diverse scenarios and use cases. Although they are more typically used for *generative* tasks (e.g., text generation, text summarization, open-ended Question Answering), they can also be employed in *discriminative* tasks.\n",
        "\n",
        "In this practice, we will use a Large Language Model to address an intent detection task. Rather than using a pre-trained encoder-only model without (Question 5) or with fine-tuning (Question 6), we will ask the LLM to classify the intent given a sentence of interest.\n",
        "<br><br>\n",
        "For now, we will use the [Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) 3.8B model, i.e., `microsoft/Phi-4-mini-instruct`."
      ],
      "metadata": {
        "id": "QgxMwtDiePfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM Prompting\n",
        "To interact with LLMs, we need to define a **prompt**, which is a piece of text containing the instruction or question we want to give or ask the model. As we saw in Practice 1 (Exercise 11), a prompt can both include only the instruction/question for the LLM but also additional information (i.e., *context*) which can be exploited by the model to generate the answer.\n",
        "\n",
        "For now, we will ask the model to classify the intent of a given sentence <u>without providing</u> any additional context. Please note that, since we want to (potentially) limit the choice of the LLM to a predefined list of intents (i.e., the set of labels of our dataset), we will also provide this list to the model.\\\n",
        "*Example of prompt*:\\\n",
        "Which is the intent of the following sentence?\\\n",
        "Choose among: label1, label2, ...\n",
        "<br><br>\n",
        "\n",
        "**Hint**: For those data instances whose intent is the concatenation of two intents (i.e., `atis_flight#atis_airfare`), consider only the first one.\n",
        "\n",
        "<u>Suggestion</u>: To increase speed, switch to a GPU runtime. You can do this by clicking on Runtime → Change runtime type → Hardware accelerator → Select T4 GPU.\\\n",
        "If you encounter an `OutOfMemoryError`, try restarting the session by clicking on Runtime → Restart session."
      ],
      "metadata": {
        "id": "yc1Nz6DueRm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "MnMuuWo-eUqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few-Shot Learning\n",
        "When we provide the LLM with additional context to be leveraged for generating an answer, this is known as *in-context learning* (ICL).\\\n",
        "A common technique to perform ICL involves including one or more **additional examples** of questions (or instructions) and their expected answers in the prompt.\\\n",
        "This can be useful for several reasons: telling the model the required output format, both in terms of structure and style, making the model reason about the provided examples to better adapt to the (new) task, tailoring model's responses to a user's specific needs, ...\\\n",
        "All of this is done directly at inference, without the need of further training the model, simply by leveraging the reasoning and generalization capabilities of LLMs.\\\n",
        "When no examples are provided, as in the previous point of the exercise, we talk about **zero-shot** learning. Conversely, when we supply additional input-output examples in the model's prompt, we talk about **few-shot** learning or $k$-shot learning, where $k$ represents the number of ICL examples.\\\n",
        "Examples can be selected according to different strategies, either randomly or in a more clever way, but please note that they <u>must be chosen from the training set</u> to avoid *data leakage*.\n",
        "<br><br>\n",
        "\n",
        "Now, try to implement few-shot learning by modifying the previous point of the exercise so that the prompt can include $k$ additional examples."
      ],
      "metadata": {
        "id": "r4HtEnZpeXSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "dCDzc5bqeZ9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different $k$ values, e.g., $k \\in [1, 3, 5, 10]$, and see if and how the model's performance changes."
      ],
      "metadata": {
        "id": "UTozppkMeaT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "ogVHq4AkeanX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now, we have used the [Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) 3.8B model, i.e., `microsoft/Phi-4-mini-instruct`, but there are plenty of LLMs!\\\n",
        "Try another model of your choice, e.g., [Mistral-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) 7B `mistralai/Mistral-7B-Instruct-v0.3`. Remember to always keep in mind the GPU memory constraints you have.\n",
        "<br><br>\n",
        "\n",
        "**Hint**: To download and use certain models, it may be needed to request access to them (e.g., [Llama3.2-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) 3B). This can be done from the Hugging Face model page after logging in. Once access is granted, you need to create your personal access token (go to your HF profile, Settings → Access Tokens → Create new token) and authenticate using it when downloading both the model and the tokenizer.\\\n",
        "To authenticate, you can either use the additional `token` parameter in the `from_pretrained()` method or via the [HF Command Line Interface](https://huggingface.co/docs/huggingface_hub/guides/cli).\n",
        "```\n",
        "model_tag = 'MODEL_TAG'\n",
        "HF_TOKEN = 'MY_HF_TOKEN'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_tag, torch_dtype=torch.float16, token=HF_TOKEN, device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_tag, token=HF_TOKEN)\n",
        "```"
      ],
      "metadata": {
        "id": "ZivPCYgxed6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "zElQT4t4ef5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing the performance of the different settings and/or models, here you can find some questions to reason about the results:\n",
        "- What is the performance in the zero-shot learning setting? (i.e., $k=0$)\n",
        "- How does it change when providing one additional example? (i.e., $k=1$)\n",
        "- What happens when increasing the number of ICL examples?\n",
        "- If you tested additional models, which one performs best in the zero-shot and few-shot learning settings?\n",
        "- What challenges or limitations did you observe?"
      ],
      "metadata": {
        "id": "qcjsfn4Jehap"
      }
    }
  ]
}