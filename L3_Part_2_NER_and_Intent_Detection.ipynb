{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/L3_Part_2_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Giuseppe Gallipoli\n",
        "\n",
        "**Credits:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entity Recognition (part 1) & Intent Detection (part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrJbaVgi_ja"
      },
      "source": [
        "## Intent Detection\n",
        "\n",
        "In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of his/her behavior in interaction with a computer system, such as in search engines. Intent Detection is the identification and categorization of what a user online intended or wanted to find when they type or speak with a conversational agent (or a search engine).\n",
        "\n",
        "![https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png](https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png)\n",
        "\n",
        "In this section, you will use the ATIS dataset: https://github.com/yvchen/JointSLU; https://www.kaggle.com/siddhadev/atis-dataset-clean/home\n",
        "\n",
        "The task is to classify the intent of a sentence. The dataset is split into train, validation and test sets. **Use the provided splits** to train and evaluate your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L6-2ABir0yS"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nJxxjTj8Q9"
      },
      "source": [
        "### **Question 5: Two-step classification model**\n",
        "\n",
        "Train a classification model to identify the intent from a given sentence. The model is required to leverage on pre-trained BERT model to generate sentence embeddings (important: **no fine-tuning**). The model is required to use the embeddings to perform classification.\n",
        "\n",
        "Once extracted the embeddings, you can use any classifier you want. For example, you can use a linear classifier (e.g., Logistic Regression) or a neural network (e.g., MLP). For your convenience, you can use the `sklearn` library for training the classifier (https://scikit-learn.org/stable/supervised_learning.html).\n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true)\n",
        "\n",
        "\n",
        "Assess the performance of the trained model (the model on top of BERT) on the test set by using the **classification accuracy**, **precision**, **recall** and **F1-score**. You can use the `sklearn` library for computing the metrics (https://scikit-learn.org/stable/api/sklearn.metrics.html).\n",
        "\n",
        "\n",
        "Note: you can use the `sentence-transformers` library to generate sentence embeddings (https://www.sbert.net/docs/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKxa9aEsEZSk"
      },
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "TEXT_COLUMN = 'tokens'\n",
        "LABEL_COLUMN = 'intent'\n",
        "\n",
        "try:\n",
        "  # Load training and test data using pandas\n",
        "  train_df = pd.read_csv(\"atis.train.csv\")\n",
        "  test_df = pd.read_csv(\"atis.test.csv\")\n",
        "  # We also load the dev (validation) set, though we won't use it in this\n",
        "  # specific train/test script.\n",
        "  dev_df = pd.read_csv(\"atis.dev.csv\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\"=\"*50)\n",
        "  print(\"ERROR: Could not find data files.\")\n",
        "  print(\"Please make sure 'atis.train.csv', 'atis.dev.csv', and 'atis.test.csv' \")\n",
        "  print(\"are in the same directory as this script.\")\n",
        "  print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ZQBBUDMO6xYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "nOXq2e4O1bdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = train_df[TEXT_COLUMN].tolist()\n",
        "y_train = train_df[LABEL_COLUMN].tolist()\n",
        "\n",
        "sentences_test = test_df[TEXT_COLUMN].tolist()\n",
        "y_test = test_df[LABEL_COLUMN].tolist()\n",
        "\n",
        "num_train = len(sentences_train)\n",
        "num_test = len(sentences_test)\n",
        "num_classes = len(set(y_train)) # Get the number of unique intents\n",
        "\n",
        "print(f\"Loaded {num_train} training sentences from 'atis.train.csv'.\")\n",
        "print(f\"Loaded {num_test} test sentences from 'atis.test.csv'.\")\n",
        "print(f\"Found {num_classes} unique intent classes in the training data.\")\n",
        "print(\"-\" * 30, \"\\n\")"
      ],
      "metadata": {
        "id": "uKtvs6bE1qi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(y_train)"
      ],
      "metadata": {
        "id": "_4IINce218_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Sentence Embeddings (no-fine-tuning)"
      ],
      "metadata": {
        "id": "cNbrtOin3ZEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on all-MiniLM-L6-v2:\n",
        "\n",
        "1. What is all-MiniLM-L6-v2?\n",
        "\n",
        "It is a Transformer model, BERT based. The \"LM\" stands for \"Language Model.\" It was created by Microsoft using a process called \"distillation\" from a much larger model. Think of it as a small, fast, and highly optimized version of BERT.\n",
        "\n",
        "SentenceTransformer Model: This specific model has been further trained by the SentenceTransformer library (using a siamese network structure) specifically for the task of creating high-quality sentence embeddings.\n",
        "\n",
        "2. Why Not Use bert-base-uncased Directly?\n",
        "\n",
        "This is the most important part, and it relates directly to your \"no fine-tuning\" constraint.\n",
        "\n",
        "A standard BERT model (like bert-base-uncased from the transformers library) is a great base for fine-tuning.\n",
        "\n",
        "However, if you just take its raw output embeddings without any fine-tuning (for example, by taking the [CLS] token's embedding), they are generally not very good for representing the meaning of a whole sentence. The model's pre-training (Masked Language Model) doesn't teach it to create good sentence-level representations by default.\n",
        "\n",
        "You would get embeddings, but they would likely lead to poor classification accuracy."
      ],
      "metadata": {
        "id": "bjMObzx4305i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# Convert sentences to vector representations (embeddings)\n",
        "print(\"Encoding training sentences... \")\n",
        "X_train_embeddings = embedder.encode(sentences_train, show_progress_bar=True)\n",
        "print(\"Encoding test sentences...\")\n",
        "X_test_embeddings = embedder.encode(sentences_test, show_progress_bar=True)\n",
        "\n",
        "print(f\"Embeddings generated. Shape of training embeddings: {X_train_embeddings.shape}\")\n",
        "print(f\"Embeddings generated. Shape of test embeddings: {X_test_embeddings.shape}\")\n",
        "print(\"-\" * 30, \"\\n\")"
      ],
      "metadata": {
        "id": "PKIA9_F23YvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification"
      ],
      "metadata": {
        "id": "TcGtjzTv4o7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A note on sklearn LogisticRegression:\n",
        "\n",
        "While its name comes from logistic (binary) regression, the scikit-learn implementation of LogisticRegression handles multi-class classification problems automatically.\n",
        "\n",
        "It does this using one of two main strategies:\n",
        "\n",
        "1. One-vs-Rest (OvR) (Default): This is the default method. If you have N classes (e.g., \"greeting\", \"goodbye\", \"order_pizza\"), it trains N separate binary logistic regression classifiers:\n",
        "\n",
        "    * One classifier to distinguish \"greeting\" (class 1) from \"all other classes\" (class 0).\n",
        "\n",
        "    * One classifier to distinguish \"goodbye\" (class 1) from \"all other classes\" (class 0).\n",
        "\n",
        "    * One classifier to distinguish \"order_pizza\" (class 1) from \"all other classes\" (class 0). When you give it a new sentence, it runs all N classifiers and picks the class that gives the highest confidence score.\n",
        "\n",
        "2. Multinomial (Softmax): You can also explicitly tell it to use this method by setting multi_class='multinomial'. This trains a single classifier model that directly outputs a probability for every class, all summing to 1. This is often called Softmax Regression."
      ],
      "metadata": {
        "id": "FPlXAjMa5lBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training Classifier"
      ],
      "metadata": {
        "id": "bazPn8Bb47TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# We increase max_iter as the dataset is larger and may need more iterations to converge.\n",
        "classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "print(\"Training classifier...\")\n",
        "# Train the classifier on the *training* embeddings and labels\n",
        "classifier.fit(X_train_embeddings, y_train)\n",
        "\n",
        "print(\"Classifier training complete.\")\n",
        "print(\"-\" * 30, \"\\n\")"
      ],
      "metadata": {
        "id": "EmkuTrfv4svO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Eval Classifier"
      ],
      "metadata": {
        "id": "xNwafc2V4_pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test_embeddings)\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "\n",
        "# We use average='weighted' to account for class imbalance (some intents\n",
        "# are more common than others).\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\n--- Overall Metrics (Weighted Average) ---\")\n",
        "print(f\"**Accuracy:** {accuracy:.4f}\")\n",
        "print(f\"**Precision:** {precision:.4f}\")\n",
        "print(f\"**Recall:** {recall:.4f}\")\n",
        "print(f\"**F1-Score:** {f1:.4f}\")\n",
        "\n",
        "# --- Detailed Classification Report ---\n",
        "# This is now much more useful as it shows performance per-intent\n",
        "print(\"\\n--- Detailed Classification Report (Per-Intent) ---\")\n",
        "# We set `labels` to the unique classes found in the test set to ensure\n",
        "# the report is complete and in a consistent order.\n",
        "unique_labels = sorted(list(set(y_test)))\n",
        "report = classification_report(y_test, y_pred, labels=unique_labels, zero_division=0)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "oqgCfXVq5JQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xrNtQTq5WW"
      },
      "source": [
        "### **Question 6: Fine-tuning end-to-end classification model**\n",
        "\n",
        "Another approach is to fine-tune the BERT model for the classification task. A classification head is added on top of the pre-trained BERT model. The classification head is trained end-to-end with the BERT model.\n",
        "This approach is more effective than the previous one because the model is trained end-to-end. However, the model requires more training time and resources.\n",
        "\n",
        "Train a new BERT model for the task of [sequence classification](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification) (include BERT fine-tuning).  \n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true)\n",
        "\n",
        "Assess the performance of the generated model by using the same metrics used in the previous question.\n",
        "\n",
        "Which model has better performance? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlKATw5sJAY"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7: Intent Detection using Large Language Models**\n",
        "**Credits:** Giuseppe Gallipoli\n",
        "\n",
        "#### Introduction\n",
        "[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) are a type of deep learning model capable of language generation. These models are built on deep learning architectures, primarily using neural networks, and are trained on massive amounts of text data. LLMs generally leverage the *Transformer* architecture, which allows them to process language in context, capturing complex relationships between words and concepts.\n",
        "\n",
        "Large Language Models have demonstrated excellent capabilities across a wide variety of tasks, making them versatile models which can be applied in diverse scenarios and use cases. Although they are more typically used for *generative* tasks (e.g., text generation, text summarization, open-ended Question Answering), they can also be employed in *discriminative* tasks.\n",
        "\n",
        "In this practice, we will use a Large Language Model to address an intent detection task. Rather than using a pre-trained encoder-only model without (Question 5) or with fine-tuning (Question 6), we will ask the LLM to classify the intent given a sentence of interest.\n",
        "<br><br>\n",
        "For now, we will use the [Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) 3.8B model, i.e., `microsoft/Phi-4-mini-instruct`."
      ],
      "metadata": {
        "id": "QgxMwtDiePfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM Prompting\n",
        "To interact with LLMs, we need to define a **prompt**, which is a piece of text containing the instruction or question we want to give or ask the model. As we saw in Practice 1 (Exercise 11), a prompt can both include only the instruction/question for the LLM but also additional information (i.e., *context*) which can be exploited by the model to generate the answer.\n",
        "\n",
        "For now, we will ask the model to classify the intent of a given sentence <u>without providing</u> any additional context. Please note that, since we want to (potentially) limit the choice of the LLM to a predefined list of intents (i.e., the set of labels of our dataset), we will also provide this list to the model.\\\n",
        "*Example of prompt*:\\\n",
        "Which is the intent of the following sentence?\\\n",
        "Choose among: label1, label2, ...\n",
        "<br><br>\n",
        "\n",
        "**Hint**: For those data instances whose intent is the concatenation of two intents (i.e., `atis_flight#atis_airfare`), consider only the first one.\n",
        "\n",
        "<u>Suggestion</u>: To increase speed, switch to a GPU runtime. You can do this by clicking on Runtime → Change runtime type → Hardware accelerator → Select T4 GPU.\\\n",
        "If you encounter an `OutOfMemoryError`, try restarting the session by clicking on Runtime → Restart session."
      ],
      "metadata": {
        "id": "yc1Nz6DueRm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "MnMuuWo-eUqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few-Shot Learning\n",
        "When we provide the LLM with additional context to be leveraged for generating an answer, this is known as *in-context learning* (ICL).\\\n",
        "A common technique to perform ICL involves including one or more **additional examples** of questions (or instructions) and their expected answers in the prompt.\\\n",
        "This can be useful for several reasons: telling the model the required output format, both in terms of structure and style, making the model reason about the provided examples to better adapt to the (new) task, tailoring model's responses to a user's specific needs, ...\\\n",
        "All of this is done directly at inference, without the need of further training the model, simply by leveraging the reasoning and generalization capabilities of LLMs.\\\n",
        "When no examples are provided, as in the previous point of the exercise, we talk about **zero-shot** learning. Conversely, when we supply additional input-output examples in the model's prompt, we talk about **few-shot** learning or $k$-shot learning, where $k$ represents the number of ICL examples.\\\n",
        "Examples can be selected according to different strategies, either randomly or in a more clever way, but please note that they <u>must be chosen from the training set</u> to avoid *data leakage*.\n",
        "<br><br>\n",
        "\n",
        "Now, try to implement few-shot learning by modifying the previous point of the exercise so that the prompt can include $k$ additional examples."
      ],
      "metadata": {
        "id": "r4HtEnZpeXSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "dCDzc5bqeZ9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different $k$ values, e.g., $k \\in [1, 3, 5, 10]$, and see if and how the model's performance changes."
      ],
      "metadata": {
        "id": "UTozppkMeaT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "ogVHq4AkeanX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now, we have used the [Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) 3.8B model, i.e., `microsoft/Phi-4-mini-instruct`, but there are plenty of LLMs!\\\n",
        "Try another model of your choice, e.g., [Mistral-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) 7B `mistralai/Mistral-7B-Instruct-v0.3`. Remember to always keep in mind the GPU memory constraints you have.\n",
        "<br><br>\n",
        "\n",
        "**Hint**: To download and use certain models, it may be needed to request access to them (e.g., [Llama3.2-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) 3B). This can be done from the Hugging Face model page after logging in. Once access is granted, you need to create your personal access token (go to your HF profile, Settings → Access Tokens → Create new token) and authenticate using it when downloading both the model and the tokenizer.\\\n",
        "To authenticate, you can either use the additional `token` parameter in the `from_pretrained()` method or via the [HF Command Line Interface](https://huggingface.co/docs/huggingface_hub/guides/cli).\n",
        "```\n",
        "model_tag = 'MODEL_TAG'\n",
        "HF_TOKEN = 'MY_HF_TOKEN'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_tag, torch_dtype=torch.float16, token=HF_TOKEN, device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_tag, token=HF_TOKEN)\n",
        "```"
      ],
      "metadata": {
        "id": "ZivPCYgxed6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "zElQT4t4ef5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing the performance of the different settings and/or models, here you can find some questions to reason about the results:\n",
        "- What is the performance in the zero-shot learning setting? (i.e., $k=0$)\n",
        "- How does it change when providing one additional example? (i.e., $k=1$)\n",
        "- What happens when increasing the number of ICL examples?\n",
        "- If you tested additional models, which one performs best in the zero-shot and few-shot learning settings?\n",
        "- What challenges or limitations did you observe?"
      ],
      "metadata": {
        "id": "qcjsfn4Jehap"
      }
    }
  ]
}