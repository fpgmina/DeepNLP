{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/L5_Part_1_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOEr39Bst_5"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "\n",
        "---\n",
        "**Teaching Assistant:** Ali Yassine\n",
        "\n",
        "**Credits:** Moreno La Quatra\n",
        "\n",
        "**Practice 5:** Machine Translation - Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9F11ek-0L8g"
      },
      "source": [
        "## **Machine Translation**\n",
        "\n",
        "Machine Translation is a sub-field of Natural Language Processing that aims at translating a text from a source language to a target language. In this practice, we will experiment with a Transformer-based model for Machine Translation. Specifically, we will benchmark the performance of a pre-trained MT model on Italian-English and English-Italian translation tasks.\n",
        "\n",
        "![](https://www.deepl.com/img/press/desktop_ENIT_2020-01.png)\n",
        "\n",
        "In this practice we will use a data collection provided by [tatoeba](https://tatoeba.org/). The following cell download a subset of the data collection, containing parallel Italian-English sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41mtAy092sCC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/train_it_en.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/test_it_en.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCTUyoq2rZo"
      },
      "source": [
        "### **Question 1: Parsing data**\n",
        "\n",
        "The first step is to parse the data collection to generate a list of sentence pairs. The data are provided in `tsv` format, where each line contains a sentence pair in the following format:\n",
        "\n",
        "`<source_language_sentence>\\t<target_language_sentence>\\n`\n",
        "\n",
        "You are provided with a training and a test set. For this question you should parse both data splits and store them in your preferred data structure.\n",
        "\n",
        "**Note:** store train and test set into separate data objects."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def parse_sentence_pairs(file_path):\n",
        "    \"\"\"\n",
        "    Parses a tab-separated file to extract sentence pairs.\n",
        "\n",
        "    Each line is expected to be in the format:\n",
        "    <source_language_sentence>\\t<target_language_sentence>\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the .tsv file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, where each tuple contains\n",
        "              (source_sentence, target_sentence).\n",
        "    \"\"\"\n",
        "    sentence_pairs = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                # Remove leading/trailing whitespace and split by tab\n",
        "                parts = line.strip().split('\\t')\n",
        "\n",
        "                # Ensure the line is valid (contains exactly two parts)\n",
        "                if len(parts) == 2:\n",
        "                    source_sentence = parts[0]\n",
        "                    target_sentence = parts[1]\n",
        "                    sentence_pairs.append((source_sentence, target_sentence))\n",
        "                else:\n",
        "                    # Optional: log a warning for malformed lines\n",
        "                    if line.strip(): # Ignore empty lines silently\n",
        "                        print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found.\")\n",
        "        return [] # Return an empty list on error\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading '{file_path}': {e}\")\n",
        "        return [] # Return an empty list on error\n",
        "\n",
        "    return sentence_pairs\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# In a real scenario, you would have these files.\n",
        "# We'll create dummy files here just to make the example runnable.\n",
        "DUMMY_FILES = {\n",
        "    'train.tsv': (\n",
        "        \"Hello world.\\tHallo Welt.\\n\"\n",
        "        \"How are you?\\tWie geht es dir?\\n\"\n",
        "        \"This is a training set.\\tDies ist ein Trainingssatz.\\n\"\n",
        "    ),\n",
        "    'test.tsv': (\n",
        "        \"This is a test.\\tDas ist ein Test.\\n\"\n",
        "        \"Goodbye.\\tAuf Wiedersehen.\\n\"\n",
        "    )\n",
        "}\n",
        "\n",
        "for a_file, content in DUMMY_FILES.items():\n",
        "    if not os.path.exists(a_file):\n",
        "        with open(a_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "# Define the paths to your data files\n",
        "train_file = 'train.tsv'\n",
        "test_file = 'test.tsv'\n",
        "\n",
        "# 1. Parse the training data\n",
        "train_data = parse_sentence_pairs(train_file)\n",
        "\n",
        "# 2. Parse the test data\n",
        "test_data = parse_sentence_pairs(test_file)\n",
        "\n",
        "# --- Display Results ---\n",
        "\n",
        "print(f\"--- Training Data (from {train_file}) ---\")\n",
        "print(f\"Total pairs: {len(train_data)}\")\n",
        "for i, (source, target) in enumerate(train_data):\n",
        "    print(f\"  Pair {i+1}: ('{source}', '{target}')\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "print(f\"--- Test Data (from {test_file}) ---\")\n",
        "print(f\"Total pairs: {len(test_data)}\")\n",
        "for i, (source, target) in enumerate(test_data):\n",
        "    print(f\"  Pair {i+1}: ('{source}', '{target}')\")"
      ],
      "metadata": {
        "id": "zLzi5cjdIgAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyW-AESi3yYe"
      },
      "source": [
        "### **Question 2: Pre-trained MT models**\n",
        "\n",
        "Pre-trained MT models are released to the public to allow researchers to experiment with them. In this question you will load a pre-trained MT model and use it to translate sentences from Italian to English and vice-versa.\n",
        "\n",
        "[EasyNMT](https://github.com/UKPLab/EasyNMT) is a Python library that provides an easy-to-use interface to pre-trained MT models. It provides a simple wrapper over HuggingFace transformers library for machine translation. In this question you will use EasyNMT to load a pre-trained MT model and translate sentences from Italian to English and vice-versa:\n",
        "\n",
        "- Load the pre-trained model for a specific direction (e.g., Italian-English or English-Italian)\n",
        "- Translate all the sentences in the test set from the source language to the target language.\n",
        "\n",
        "\n",
        "**Note 1**: the choice for the MT model is up to you.\n",
        "\n",
        "**Note 2**: store the translated sentences in both directions using the data structure of your choice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install easynmt sacremoses, nltk"
      ],
      "metadata": {
        "id": "vrSfg-lyIv6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install easynmt sacremoses nltk"
      ],
      "metadata": {
        "id": "oT-Wfz5IeKqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from easynmt import EasyNMT\n",
        "\n",
        "# --- 1. Setup: Download NLTK dependency and Load Model ---\n",
        "\n",
        "# FIX: Download the required 'punkt' tokenizer model\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"NLTK 'punkt' resource not found. Downloading...\")\n",
        "    nltk.download('punkt')\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# We'll re-create a dummy 'test_data' list as parsed in Question 1.\n",
        "test_data = [\n",
        "    (\"Questa è una prova.\", \"This is a test.\"),\n",
        "    (\"Il cielo è blu.\", \"The sky is blue.\"),\n",
        "    (\"Mi piace la pizza.\", \"I like pizza.\"),\n",
        "    (\"L'intelligenza artificiale sta cambiando il mondo.\", \"Artificial intelligence is changing the world.\")\n",
        "]\n",
        "\n",
        "print(\"\\nOriginal test_data (it -> en):\")\n",
        "for pair in test_data:\n",
        "    print(f\"  {pair}\")\n",
        "\n",
        "# Separate the source (Italian) and target (English) sentences\n",
        "source_it_sentences = [pair[0] for pair in test_data]\n",
        "target_en_sentences = [pair[1] for pair in test_data]\n",
        "\n",
        "# Load the 'opus-mt' model.\n",
        "print(\"\\nLoading EasyNMT model 'opus-mt'...\")\n",
        "model = EasyNMT('opus-mt')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- 2. Translate: Italian to English (it-en) ---\n",
        "\n",
        "print(\"\\n--- Translating Italian to English ---\")\n",
        "translated_it_to_en = model.translate(source_it_sentences,\n",
        "                                      source_lang='it',\n",
        "                                      target_lang='en')\n",
        "\n",
        "\n",
        "# --- 3. Translate: English to Italian (en-it) ---\n",
        "\n",
        "print(\"\\n--- Translating English to Italian ---\")\n",
        "translated_en_to_it = model.translate(target_en_sentences,\n",
        "                                      source_lang='en',\n",
        "                                      target_lang='it')\n",
        "\n",
        "\n",
        "# --- 4. Display Results ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*45)\n",
        "print(\"     RESULTS: Italian -> English Translations\")\n",
        "print(\"=\"*45)\n",
        "for i in range(len(test_data)):\n",
        "    print(f\"  Original (it):   '{source_it_sentences[i]}'\")\n",
        "    print(f\"  Reference (en):  '{target_en_sentences[i]}'\")\n",
        "    print(f\"  Translated (en): '{translated_it_to_en[i]}'\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*45)\n",
        "print(\"     RESULTS: English -> Italian Translations\")\n",
        "print(\"=\"*45)\n",
        "for i in range(len(test_data)):\n",
        "    print(f\"  Original (en):   '{target_en_sentences[i]}'\")\n",
        "    print(f\"  Reference (it):  '{source_it_sentences[i]}'\")\n",
        "    print(f\"  Translated (it): '{translated_en_to_it[i]}'\\n\")"
      ],
      "metadata": {
        "id": "SvTAPXNEIxXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv-4koeH6yvt"
      },
      "source": [
        "### **Question 3: BLEU and METEOR scores**\n",
        "\n",
        "In this question you will evaluate the performance of your machine translation (MT) model using **two** evaluation metrics: **[BLEU evaluation metric](https://github.com/mjpost/sacrebleu)** and **[METEOR evaluation metric](https://huggingface.co/spaces/evaluate-metric/meteor)**. You **must** compute and report scores for both translation directions: `EN→IT` and `IT→EN`.\n",
        "\n",
        "---\n",
        "\n",
        "#### BLEU (Bilingual Evaluation Understudy)\n",
        "\n",
        "**BLEU** measures how much the model’s translation overlaps with a reference translation by comparing shared **n-grams** (word sequences). It gives a precision-oriented score that rewards exact word matches.\n",
        "\n",
        "- **Pros:** Fast, standardized, and good for large-scale comparisons.\n",
        "- **Cons:** Only captures exact matches, ignoring synonyms or paraphrases; may not always align with human judgment.\n",
        "\n",
        "> Use BLEU as implemented in `sacrebleu`.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
        "\n",
        "**METEOR** was developed to better reflect human judgment by allowing more flexible word matching. It aligns hypothesis and reference words using:\n",
        "\n",
        "- **Exact matches**\n",
        "- **Stem matches** (e.g., *run* ↔ *running*)\n",
        "- **Synonyms and paraphrases**\n",
        "\n",
        "It then combines these matches into a single score with penalties for disordered or fragmented output.\n",
        "\n",
        "- **Pros:** More linguistically aware; correlates better with human evaluations.\n",
        "- **Cons:** Slower to compute; depends on external lexical resources.\n",
        "\n",
        "> Compute METEOR using `evaluate` or `nltk`.\n",
        "\n",
        "---\n",
        "\n",
        "The following cell installs the `sacrebleu`, `nltk`, and `evaluate` libraries that can be used to compute these metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9dLMLRw7SDu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sacrebleu evaluate nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "import evaluate\n",
        "import nltk\n",
        "\n",
        "# --- 1. Load the METEOR metric ---\n",
        "# This will also trigger downloads for 'wordnet' and 'omw-1.4'\n",
        "# if NLTK doesn't have them, as METEOR relies on them.\n",
        "print(\"Loading METEOR metric...\")\n",
        "try:\n",
        "    meteor_metric = evaluate.load('meteor')\n",
        "except LookupError:\n",
        "    print(\"METEOR dependencies not found. Downloading 'wordnet' and 'omw-1.4'...\")\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "    meteor_metric = evaluate.load('meteor')\n",
        "print(\"METEOR metric loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- 2. Evaluate: Italian to English (IT -> EN) ---\n",
        "print(\"\\n--- Evaluating: Italian to English (IT -> EN) ---\")\n",
        "\n",
        "# Define the model's outputs (hypotheses) and the correct answers (references)\n",
        "hypotheses = translated_it_to_en\n",
        "references = target_en_sentences\n",
        "\n",
        "# BLEU (sacrebleu)\n",
        "# sacrebleu requires references to be in a list of lists, e.g., [[ref1, ref2], [ref1, ref2]]\n",
        "# For a single reference set, we wrap it: [references]\n",
        "it_en_bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
        "print(f\"  BLEU Score: {it_en_bleu.score:.2f}\")\n",
        "\n",
        "# METEOR (evaluate)\n",
        "# 'evaluate' is simpler and can take a flat list of references\n",
        "it_en_meteor = meteor_metric.compute(predictions=hypotheses,\n",
        "                                     references=references)\n",
        "print(f\"  METEOR Score: {it_en_meteor['meteor']:.4f}\")\n",
        "\n",
        "\n",
        "# --- 3. Evaluate: English to Italian (EN -> IT) ---\n",
        "print(\"\\n--- Evaluating: English to Italian (EN -> IT) ---\")\n",
        "\n",
        "# Define the new hypotheses and references\n",
        "hypotheses = translated_en_to_it\n",
        "references = source_it_sentences\n",
        "\n",
        "# BLEU (sacrebleu)\n",
        "en_it_bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
        "print(f\"  BLEU Score: {en_it_bleu.score:.2f}\")\n",
        "\n",
        "# METEOR (evaluate)\n",
        "en_it_meteor = meteor_metric.compute(predictions=hypotheses,\n",
        "                                     references=references)\n",
        "print(f\"  METEOR Score: {en_it_meteor['meteor']:.4f}\")\n",
        "\n",
        "# --- 4. Final Summary ---\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"      FINAL RESULTS\")\n",
        "print(\"=\"*30)\n",
        "print(f\"| Direction | Metric  | Score  |\")\n",
        "print(f\"|-----------|---------|--------|\")\n",
        "print(f\"| IT -> EN  | BLEU    | {it_en_bleu.score:<6.2f} |\")\n",
        "print(f\"| IT -> EN  | METEOR  | {it_en_meteor['meteor']:<6.4f} |\")\n",
        "print(f\"| EN -> IT  | BLEU    | {en_it_bleu.score:<6.2f} |\")\n",
        "print(f\"| EN -> IT  | METEOR  | {en_it_meteor['meteor']:<6.4f} |\")"
      ],
      "metadata": {
        "id": "srGLkpLoJOxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4: Comparison with another Pre-trained MT Model**\n",
        "\n",
        "In this question, you will experiment with another pre-trained MT model and compare its performance with the model used in Question 2.\n",
        "\n",
        "Follow the same translation procedure as before (EN→IT and IT→EN) and evaluate the results using the BLEU and METEOR metrics from Question 3.\n",
        "\n",
        "Use [EasyNMT](https://github.com/UKPLab/EasyNMT) to load and run the pre-trained model.\n"
      ],
      "metadata": {
        "id": "VoR_OKc5ghQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "HSmLo9KcrF9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a88d8d275dc8276b143b02757b297c7b7ccc4199bd118b2f9ce33906ca7c97c0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}