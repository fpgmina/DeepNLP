{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/L3_Part_1_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Giuseppe Gallipoli\n",
        "\n",
        "**Credits:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entity Recognition (part 1) & Intent Detection (part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQde2M-U4KV"
      },
      "source": [
        "## Named Entity Recognition (NER)\n",
        "\n",
        "The Named Entity Recognition task aims at identifying and classifying named entities in a text. Named entities are real-world objects such as persons, locations, organizations, etc. The task takes as input a sentence and determines the boundaries of the named entities and their type.\n",
        "\n",
        "For example, given the sentence:\n",
        "\n",
        "```\n",
        "I went to Paris last week.\n",
        "```\n",
        "\n",
        "the task is to identify the named entity `Paris` as a location.\n",
        "\n",
        "Hereafter an illustration of the NER task:\n",
        "\n",
        "![https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg](https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VXjktGnWUTm"
      },
      "source": [
        "In the first part of this practice, you will:\n",
        "- explore the NER task using pre-trained models available on spaCy and HuggingFace\n",
        "- evaluate the performance of a spaCy NER model on a custom dataset\n",
        "- evaluate the performance of a HuggingFace NER model on a custom dataset\n",
        "\n",
        "NB: The library used to evaluate the performance of the models is `seqeval`, which is a library for evaluating sequence labeling tasks, or `eval4ner` for the NER task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9elJruvpWAQ_"
      },
      "source": [
        "### **Question 1: Data preparation**\n",
        "\n",
        "The first step is to prepare the data. In this practice, you will use the WikiGold dataset [1][2], which is a collection of annotated sentences from Wikipedia. The dataset is available in [CONLL](https://simpletransformers.ai/docs/ner-data-formats/#text-file-in-conll-format) format. The dataset is available [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt).\n",
        "\n",
        "**Please, read carefully the following instructions before starting to work on the practice.**\n",
        "\n",
        "You need to extract clean sentences (no annotation) and, for each sentence, the corresponding annotations. The dataset has the following format:\n",
        "\n",
        "- `sentences`: list of sentences\n",
        "- `annotations`: list of list of entities (both string and class information). E.g., `[[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')], [('Osc-Dis', 'MISC'), ('Introduction 010', 'MISC'), ('Come', 'MISC')], ...]`. You can remove I- prefix because the data collection does not actually contain valuable prefixes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[1] Balasuriya, Dominic, et al. \"Named entity recognition in wikipedia.\"\n",
        "    Proceedings of the 2009 Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources. Association for Computational Linguistics, 2009.\n",
        "\n",
        "[2] Nothman, Joel, et al. \"Learning multilingual named entity recognition\n",
        "    from Wikipedia.\" Artificial Intelligence 194 (2013): 151-175\n",
        "\n",
        "---\n",
        "\n",
        "The following cell downloads the dataset on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeGD-RtlV_0k"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid spending too much time on data processing, the following cells prepare the dataset for you.\n",
        "After running the cell, you will have the following variables:\n",
        "- `sentences_with_labels`: a list of tokens with their corresponding labels\n",
        "- `sentences`: a list of the sentences in the dataset\n",
        "- `labels`: a list of lists of labels. Each element in the outer list corresponds to a list of labels for a sentence in the dataset.\n",
        "\n",
        "Hereafter an example of the provided data:\n",
        "\n",
        "```python\n",
        "sentences_with_labels[0] = [\n",
        "    ['010', 'I-MISC'],\n",
        "    ['is', 'O'],\n",
        "    ['the', 'O'],\n",
        "    ['tenth', 'O'],\n",
        "    ['album', 'O'],\n",
        "    ['from', 'O'],\n",
        "    ['Japanese', 'I-MISC'],\n",
        "    ['Punk', 'O'],\n",
        "    ['Techno', 'O'],\n",
        "    ['band', 'O'],\n",
        "    ['The', 'I-ORG'],\n",
        "    ['Mad', 'I-ORG'],\n",
        "    ['Capsule', 'I-ORG'],\n",
        "    ['Markets', 'I-ORG'],\n",
        "    ['.', 'O']\n",
        "]\n",
        "\n",
        "sentences[0] = [\n",
        "    '010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets .'\n",
        "]\n",
        "\n",
        "labels[0] = [\n",
        "    ('010', 'MISC'),\n",
        "    ('Japanese', 'MISC'),\n",
        "    ('The Mad Capsule Markets', 'ORG')\n",
        "]\n",
        "```\n",
        "\n",
        "Please, note that the labels are not in IOB format. You can ignore the I- prefix because the data collection does not actually contain valuable prefixes.\n",
        "Get familar with the data by printing the first 10 sentences and their corresponding labels. Which are the labels in the dataset?"
      ],
      "metadata": {
        "id": "kKs0bKpiZ6-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install datasets\n",
        "! pip install transformers\n",
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "bC1C3ncoaCUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK19r9la4jUd"
      },
      "source": [
        "def split_text_label(filename):\n",
        "    f = open(filename)\n",
        "    split_labeled_text = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "             if len(sentence) > 0:\n",
        "                 split_labeled_text.append(sentence)\n",
        "                 sentence = []\n",
        "             continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "    if len(sentence) > 0:\n",
        "        split_labeled_text.append(sentence)\n",
        "        sentence = []\n",
        "    return split_labeled_text\n",
        "sentences_with_labels = split_text_label(\"wikigold.conll.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences_with_labels[0])"
      ],
      "metadata": {
        "id": "ZGu_oqZczMEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfne7po3w8MH"
      },
      "source": [
        "sentences = []\n",
        "\n",
        "for sent_list in sentences_with_labels:\n",
        "    sentence = [s[0] for s in sent_list]\n",
        "    sentence = \" \".join(sentence)\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (sentences[0])"
      ],
      "metadata": {
        "id": "4P1XsYDYzwiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdWH0yVG7dy0"
      },
      "source": [
        "labels = []\n",
        "overall_labels = []\n",
        "for sent_list in sentences_with_labels:\n",
        "    current_labels = []\n",
        "    prev = \"O\"\n",
        "    current_entity = \"\"\n",
        "    for w, l in sent_list:\n",
        "        overall_labels.append(l)\n",
        "        if l != \"O\" and prev != \"O\":\n",
        "            if l == prev:\n",
        "                # continue entity\n",
        "                current_entity += w + \" \"\n",
        "            else:\n",
        "                # end prev and start a new one\n",
        "                current_labels.append((current_entity.strip(), prev.split(\"-\")[1]))\n",
        "                current_entity = w + \" \"\n",
        "        elif l == \"O\" and  prev != \"O\":\n",
        "            # end prev\n",
        "            current_labels.append((current_entity.strip(), prev.split(\"-\")[1]))\n",
        "            current_entity = \"\"\n",
        "        elif l != \"O\" and prev == \"O\":\n",
        "            # start new\n",
        "            current_entity = w + \" \"\n",
        "\n",
        "        prev = l\n",
        "    labels.append(current_labels)\n",
        "\n",
        "print (labels)\n",
        "overall_labels = list(set(overall_labels))\n",
        "overall_labels = [o for o in overall_labels if o != \"O\"]\n",
        "overall_labels = [o.split(\"-\")[1] for o in overall_labels]\n",
        "print (overall_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (labels[0])"
      ],
      "metadata": {
        "id": "db8Qm8uazzie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3VJcC9Bvnf"
      },
      "source": [
        "### **Question 2: Inference with spaCy for entity recognition**\n",
        "\n",
        "spaCy is a free, open-source library for advanced Natural Language Processing in Python. It features NER models for different languages including English.\n",
        "The models are available [here](https://spacy.io/models).\n",
        "\n",
        "For this question you are asked to instantiate a spaCy model for English and perform inference on the sentences in the dataset. The English model contains a superset of the labels in the dataset. For this reason, you need to map the labels that are not in the dataset to the `MISC` label.\n",
        "\n",
        "You are expected to generate an output similar to the following:\n",
        "```python\n",
        "[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')]\n",
        "```\n",
        "\n",
        "Please pay attention to the token attributes (you can find more information [here](https://spacy.io/api/token#attributes)) and the entity attributes (you can find more information [here](https://spacy.io/api/entityrecognizer)).\n",
        "\n",
        "The following cell instantiates a spaCy model for English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFEwrJwtZA4q"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF9z2opG4x1J"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "pred_ner = []\n",
        "for i, s in enumerate(tqdm(sentences)):\n",
        "    doc = nlp(s)\n",
        "    current_ner = []\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ not in overall_labels:\n",
        "            label = \"MISC\"\n",
        "        else:\n",
        "            label = entity.label_\n",
        "        current_ner.append((entity.text, label))\n",
        "\n",
        "    pred_ner.append(current_ner)\n",
        "print (pred_ner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf0NzZzjPheL"
      },
      "source": [
        "### **Question 3: Compute metrics for evaluating NER**\n",
        "\n",
        "The output of NER models consists of a set of named entities. To evaluate the performance of a model, we need to compare the predicted named entities with the ground truth.\n",
        "\n",
        "For this question, you need to use [`eval4ner`](https://github.com/cyk1337/eval4ner) package to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACNK6UU98VZr"
      },
      "source": [
        "%%capture\n",
        "! pip install eval4ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval4ner accepts the following format:\n",
        "# [('PER', 'John Jones'), ('PER', 'Peter Peters'), ('LOC', 'York')]\n",
        "# need to switch the position of NER_type and the string\n",
        "\n",
        "labels_new = [[(label, text) for text, label in label] for label in labels]\n",
        "pred_ner_new = [[(label, text) for text, label in pred] for pred in pred_ner]"
      ],
      "metadata": {
        "id": "YQHRWAI6Abtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyNrvYyV8T_E"
      },
      "source": [
        "import eval4ner.muc as muc\n",
        "\n",
        "evaluations = muc.evaluate_all(pred_ner_new, labels_new, sentences, verbose=False)\n",
        "print()\n",
        "\n",
        "eval_types = [\"exact\", \"partial\", \"strict\", \"type\"]\n",
        "for eval in eval_types:\n",
        "    print (eval)\n",
        "    print (\"Precision: \", evaluations[eval][\"precision\"])\n",
        "    print (\"Recall   : \", evaluations[eval][\"recall\"])\n",
        "    print (\"F1_score : \", evaluations[eval][\"f1_score\"])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC19UMUZdJxc"
      },
      "source": [
        "### **Question 4: Inference with transformers pipeline**\n",
        "\n",
        "Transformer-based models can be fine-tuned for token-level classification. The task is to classify each token in a sentence and assign it to a class.\n",
        "The NER task is a token-level classification task and the models can be used for performing inference on the sentences in the dataset.\n",
        "\n",
        "You can use the pipeline available on the HuggingFace [transformers library](https://huggingface.co/docs/transformers/main_classes/pipelines). The pipeline allows to perform inference on a list of sentences.\n",
        "\n",
        "Evaluate the **standard** model using the pipeline (`pipe = pipeline(\"ner\")`). Check the documentation here: https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline\n",
        "\n",
        "\n",
        "A few notes about the question (**read carefully**):\n",
        "1. The output of the pipeline differs with respect to spaCy. Please be sure to process data correctly before running evaluation.\n",
        "2. `ignore_labels` parameter could be used to exclude labels from the prediction.\n",
        "3. `##` symbol is used when a token is a continuation of a previous one (Poli + ##TO). You may need to check this specific case to merge the tokens correctly.\n",
        "4. Use `eval4ner` to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6pQuOdTZZVo"
      },
      "source": [
        "%%capture\n",
        "! pip install transformers\n",
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KarrQxigZZS-"
      },
      "source": [
        "import datasets\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.base import KeyDataset\n",
        "import torch, tqdm\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.devivce('cpu')\n",
        "\n",
        "pipe = pipeline(\"ner\", device=device)\n",
        "pipe.ignore_labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50PGkYg3ZYx5"
      },
      "source": [
        "from tqdm import tqdm\n",
        "pred_transformers_ner = []\n",
        "overall_labels_transformers = []\n",
        "for i, s in enumerate(tqdm(sentences)):\n",
        "    out = pipe(s)\n",
        "    current_labels = []\n",
        "    prev = \"O\"\n",
        "    current_entity = \"\"\n",
        "    for o in out:\n",
        "        overall_labels_transformers.append(o['entity'])\n",
        "        l = o['entity']\n",
        "        w = o['word']\n",
        "        if l != \"O\" and prev != \"O\":\n",
        "            if l == prev:\n",
        "                # continue entity\n",
        "                current_entity += w + \" \"\n",
        "            else:\n",
        "                # end prev and start a new one\n",
        "                current_entity = current_entity.strip()\n",
        "                current_entity = current_entity.replace(\" ##\", \"\")\n",
        "                current_labels.append((current_entity, prev.split(\"-\")[1]))\n",
        "                current_entity = w + \" \"\n",
        "        elif l == \"O\" and  prev != \"O\":\n",
        "            # end prev\n",
        "            current_entity = current_entity.strip()\n",
        "            current_entity = current_entity.replace(\" ##\", \"\")\n",
        "            current_labels.append((current_entity, prev.split(\"-\")[1]))\n",
        "            current_entity = \"\"\n",
        "        elif l != \"O\" and prev == \"O\":\n",
        "            # start new\n",
        "            current_entity = w + \" \"\n",
        "\n",
        "        prev = l\n",
        "    pred_transformers_ner.append(current_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RduRilQ9llOV"
      },
      "source": [
        "print (len(labels))\n",
        "print (len(pred_transformers_ner))\n",
        "overall_labels_transformers = list(set(overall_labels_transformers))\n",
        "overall_labels_transformers = [o for o in overall_labels_transformers if o != \"O\"]\n",
        "overall_labels_transformers = [o.split(\"-\")[1] for o in overall_labels_transformers]\n",
        "print (overall_labels_transformers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval4ner accepts the following format:\n",
        "# [('PER', 'John Jones'), ('PER', 'Peter Peters'), ('LOC', 'York')]\n",
        "# need to switch the position of NER_type and the string\n",
        "\n",
        "labels_new = [[(label, text) for text, label in label] for label in labels]\n",
        "pred_transformers_ner_new = [[(label, text) for text, label in label] for label in pred_transformers_ner]"
      ],
      "metadata": {
        "id": "UP7bBXt6BMa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep91hyYyi6CN"
      },
      "source": [
        "import eval4ner.muc as muc\n",
        "\n",
        "evaluations = muc.evaluate_all(pred_transformers_ner_new, labels_new, sentences, verbose=False)\n",
        "print()\n",
        "\n",
        "eval_types = [\"exact\", \"partial\", \"strict\", \"type\"]\n",
        "for eval in eval_types:\n",
        "    print (eval)\n",
        "    print (\"Precision: \", evaluations[eval][\"precision\"])\n",
        "    print (\"Recall   : \", evaluations[eval][\"recall\"])\n",
        "    print (\"F1_score : \", evaluations[eval][\"f1_score\"])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}