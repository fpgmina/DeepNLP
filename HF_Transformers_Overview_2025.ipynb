{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/HF_Transformers_Overview_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUUwl6EYMkK2"
      },
      "source": [
        "# **Hugging Face ðŸ¤— Transformers Overview**\n",
        "---\n",
        "\n",
        "**Teaching Assistant:** Giuseppe Gallipoli\n",
        "\n",
        "**Credits:** Moreno La Quatra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XehBEZl8Nbup"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install transformers[torch]\n",
        "! pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo3AAG3PMwkO"
      },
      "source": [
        "# Pipelines\n",
        "\n",
        "The easiest way to use ðŸ¤— Transformers library is to interact with [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines). They embed all the steps required to analyze input text:\n",
        "- Pre-processing\n",
        "- Model inference\n",
        "- Post-processing\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg\" width=\"75%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmm6KCiAMiLz"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJecy8r0NXSX"
      },
      "source": [
        "## Sentiment analysis pipeline (Encoder-only models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CoB5vGQNjvX"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", device='cuda')\n",
        "res = sentiment_analyzer([\"I like Deep NLP course\", \"I don't like Deep NLP course!\"])\n",
        "print(f'\\n{res}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-H0pWCOQd0"
      },
      "outputs": [],
      "source": [
        "# providing model: look for it on Model Hub: https://huggingface.co/models\n",
        "\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\", device='cuda')\n",
        "res = sentiment_analyzer([\"TLDR: the movie was amazing\", \"What a mess! The plot was awful\"])\n",
        "print(f'\\n{res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr1CB-ONPFR5"
      },
      "source": [
        "## Text Generation pipeline (Decoder-only models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaY7lWnPPLKL"
      },
      "outputs": [],
      "source": [
        "text_generator = pipeline(\"text-generation\", device='cuda')\n",
        "res = text_generator(\"The meaning of life is\")\n",
        "print(f\"\\n{res}\\n\")\n",
        "print(res[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# providing model: look for it on Model Hub: https://huggingface.co/models\n",
        "\n",
        "text_generator = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-italian\", device='cuda')\n",
        "res = text_generator(\"Il senso della vita Ã¨\")\n",
        "print(f\"\\n{res}\\n\")\n",
        "print(res[0]['generated_text'])"
      ],
      "metadata": {
        "id": "d18NgUnjgfEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbhvVZJfPpas"
      },
      "source": [
        "## Text summarization pipeline (Encoder-Decoder models):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMf72AS6QnCu"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", device='cuda')\n",
        "res = summarizer(\"Scientific articles can be annotated with short sentences, called highlights, providing readers with an at-a-glance overview of the main findings. Highlights are usually manually specified by the authors. This paper presents a supervised approach, based on regression techniques, with the twofold aim at automatically extracting highlights of past articles with missing annotations and simplifying the process of manually annotating new articles. To this end, regression models are trained on a variety of features extracted from previously annotated articles. The proposed approach extends existing extractive approaches by predicting a similarity score, based on n-gram co-occurrences, between article sentences and highlights. The experimental results, achieved on a benchmark collection of articles ranging over heterogeneous topics, show that the proposed regression models perform better than existing methods, both supervised and not.\")\n",
        "print(f\"\\n{res}\\n\")\n",
        "print(res[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQJmAHa-REhh"
      },
      "outputs": [],
      "source": [
        "# providing model: look for it on Model Hub: https://huggingface.co/models\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"shamikbose89/mt5-small-finetuned-arxiv-cs-finetuned-arxiv-cs-full\", device='cuda')\n",
        "res = summarizer(\"Scientific articles can be annotated with short sentences, called highlights, providing readers with an at-a-glance overview of the main findings. Highlights are usually manually specified by the authors. This paper presents a supervised approach, based on regression techniques, with the twofold aim at automatically extracting highlights of past articles with missing annotations and simplifying the process of manually annotating new articles. To this end, regression models are trained on a variety of features extracted from previously annotated articles. The proposed approach extends existing extractive approaches by predicting a similarity score, based on n-gram co-occurrences, between article sentences and highlights. The experimental results, achieved on a benchmark collection of articles ranging over heterogeneous topics, show that the proposed regression models perform better than existing methods, both supervised and not.\")\n",
        "print(f\"\\n{res}\\n\")\n",
        "print(res[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEsFU8dCSX5i"
      },
      "source": [
        "# Tokenizers\n",
        "\n",
        "Text is tokenized into lexical units prior to passing to the model. Each token is mapped to an integer value that represent the token itself. Tokenizer are basically large vocabularies that allow mapping tokens into integer identifiers.\n",
        "\n",
        "[Tokenizers](https://huggingface.co/docs/transformers/master/en/main_classes/tokenizer) contains all the pre-processing tools that are used to split long text into tokens. Once trained their vocabulary is fixed (it can always be updated with additional training phases).\n",
        "\n",
        "**NB**: AutoClasses allow to generate tokenizer (and model) objects without instantiating the specific model tokenizer (and model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwbHfSg5xbh3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokens_w = tokenizer.tokenize(\"I'm learning Deep NLP\")\n",
        "print(f'{tokens_w}\\n')\n",
        "\n",
        "tokens = tokenizer (\"I'm learning Deep NLP\")\n",
        "\n",
        "print(tokens.keys())\n",
        "ids, types, attn = tokens.values()\n",
        "\n",
        "print()\n",
        "print(tokens)\n",
        "print()\n",
        "\n",
        "for i, t, a in zip(ids, types, attn):\n",
        "  print(f'token id {i:<5} -> {tokenizer.convert_ids_to_tokens(i):<10} | type = {t} | attention = {a}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITHhDF_Syy-L"
      },
      "source": [
        "Each model configuration has a maximum length of tokens that can be used for processing. It is common to process sentences that have different lenghts. In this case:\n",
        "\n",
        "- `max_length` parameter allow to set a maximum number of tokens for processing\n",
        "- `truncation` allows to enable truncation for sentences exceeding the `max_length`\n",
        "- `padding` allows to enable padding for sentences shorter than `max_length`\n",
        "\n",
        "The tokenizer return the `attention_mask` that allows the model to compute attention weights only for tokens (and not for padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhl4h2kfyxuc"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer (\"I'm learning Deep NLP\", padding='max_length', max_length=16)\n",
        "print (tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB-t9gOxz26l"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer (\"I'm learning NLP at Politecnico di Torino. I'm a MSc student\", padding='max_length', max_length=16)\n",
        "ids, types, attn = tokens.values()\n",
        "for i, t, a in zip(ids, types, attn):\n",
        "  print(f'token id {i:<5} -> {tokenizer.convert_ids_to_tokens(i):<10} | type = {t} | attention = {a}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer (\"I'm learning NLP at Politecnico di Torino. I'm a MSc student\", padding='max_length', max_length=16, truncation=True)\n",
        "ids, types, attn = tokens.values()\n",
        "for i, t, a in zip(ids, types, attn):\n",
        "  print(f'token id {i:<5} -> {tokenizer.convert_ids_to_tokens(i):<10} | type = {t} | attention = {a}')"
      ],
      "metadata": {
        "id": "7hp2rqhqebTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer (\"I'm learning NLP at Politecnico di Torino. I'm a MSc student\", padding='max_length', max_length=32, truncation=True)\n",
        "ids, types, attn = tokens.values()\n",
        "for i, t, a in zip(ids, types, attn):\n",
        "  print(f'token id {i:<5} -> {tokenizer.convert_ids_to_tokens(i):<10} | type = {t} | attention = {a}')"
      ],
      "metadata": {
        "id": "H4XfkAt5eijG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXKrg8z7xYHT"
      },
      "source": [
        "Tokenizers do not only allow encoding text to IDs, they also allow the opposite conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1mPxt4A6ZIa"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.decode(tokens.input_ids, skip_special_tokens=False)\n",
        "print (text)\n",
        "\n",
        "# [CLS] special token for encoder model, used for classification/regression tasks\n",
        "# [SEP] special token to separate multiple sentences\n",
        "# [PAD] special token for padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwgV3Zeq60hd"
      },
      "source": [
        "# Models\n",
        "\n",
        "Tranformer-based models are wrapped around their own class in ðŸ¤— Transformers. Similarly to AutoTokenizer, AutoModel class is able to take in charge the instantiation of the correct class for the model we want to use.\n",
        "\n",
        "Given that, models for specific tasks exist with the same backbone architecture (e.g., BERT can be used both for sequence classification or for token-level classification), the Auto Model should be instantiated with the correct task appended (e.g., AutoModelForSequenceClassification)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihJVBKKd7DCn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfEqQKVd7q6i"
      },
      "source": [
        "However, pre-trained BERT model is not fine-tuned for any specific task (this is the reason behind the warning). If we want to use this model, we first need to fine-tune it (or we can use another model already fine-tuned for the task).\n",
        "\n",
        "[Model Hub](https://huggingface.co/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSl1NTvG737d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "bert_model_sc = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2BETxUK8K4J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "sentences = [\"Google stocks went up suddently, I earned 30B$\"]\n",
        "tokenized_sentence = tokenizer(sentences, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=16)\n",
        "pred = bert_model_sc(**tokenized_sentence)\n",
        "classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "print (pred[0][0].detach().numpy(), np.argmax(pred[0][0].detach().numpy()), classes[np.argmax(pred[0][0].detach().numpy())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ8kYLV08Id4"
      },
      "source": [
        "# Fine-tuning a pre-trained model\n",
        "\n",
        "Pre-training + Fine-tuning paradigm is the key of the success of the ðŸ¤— Transformers library. [Model Hub](https://huggingface.co/models) contains plenty of pre-trained models that can be used as they are, or can be fine-tuned on new datasets.\n",
        "\n",
        "[Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer) allows user to easily fine-tune the selected model for the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYU2WDjtGOsh"
      },
      "outputs": [],
      "source": [
        "# Your own data\n",
        "import pandas as pd\n",
        "\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/transformers_overview/Corona_NLP_train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/transformers_overview/Corona_NLP_test.csv\n",
        "\n",
        "df_train = pd.read_csv(\"Corona_NLP_train.csv\")\n",
        "df_test  = pd.read_csv(\"Corona_NLP_test.csv\")\n",
        "\n",
        "df_train = df_train.dropna(how = 'any')\n",
        "df_test  = df_test.dropna (how = 'any')\n",
        "\n",
        "train_sentences = df_train[\"OriginalTweet\"].tolist()\n",
        "train_y = df_train[\"Sentiment\"].tolist()\n",
        "\n",
        "print(f\"Train set: {len(train_sentences)}, {len(train_y)}\")\n",
        "\n",
        "eval_samples = int(0.05*len(train_sentences))\n",
        "\n",
        "\n",
        "eval_sentences = train_sentences[:eval_samples]\n",
        "eval_y = train_y[:eval_samples]\n",
        "\n",
        "train_sentences = train_sentences[eval_samples:]\n",
        "train_y = train_y[eval_samples:]\n",
        "\n",
        "test_sentences = df_test[\"OriginalTweet\"].tolist()\n",
        "test_y = df_test[\"Sentiment\"].tolist()\n",
        "\n",
        "print(f\"Train set: {len(train_sentences)}, {len(train_y)}\")\n",
        "print(f\"Eval set: {len(eval_sentences)}, {len(eval_y)}\")\n",
        "print(f\"Test set: {len(test_sentences)}, {len(test_y)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTZ8kUU0EXTX"
      },
      "outputs": [],
      "source": [
        "# Examples for Sequence Classification\n",
        "\n",
        "# tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = len(set(train_y)))\n",
        "\n",
        "# Tokenization step\n",
        "tokenized_train = tokenizer(train_sentences, padding=\"max_length\", truncation=True, max_length=64)\n",
        "tokenized_test  = tokenizer(test_sentences, padding=\"max_length\", truncation=True, max_length=64)\n",
        "tokenized_eval  = tokenizer(eval_sentences, padding=\"max_length\", truncation=True, max_length=64)\n",
        "\n",
        "# Label encoding step\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def label_encoding(labels, le):\n",
        "    # instantiate labelencoder object\n",
        "    y = le.transform(labels)\n",
        "    return y\n",
        "\n",
        "all_labels = []\n",
        "for label in set(train_y):\n",
        "    all_labels.append(label)\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(all_labels)\n",
        "\n",
        "train_y = label_encoding(train_y, le)\n",
        "test_y = label_encoding(test_y, le)\n",
        "eval_y = label_encoding(eval_y, le)\n",
        "\n",
        "import torch\n",
        "class SCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_ds = SCDataset(tokenized_train, train_y)\n",
        "eval_ds = SCDataset(tokenized_eval, eval_y)\n",
        "test_ds = SCDataset(tokenized_test, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zgkZtrCAYU9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Compute metrics function should return a dictionary with the metrics computed for the task (e.g., accuracy)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    predictions = np.argmax(pred.predictions, axis=-1)\n",
        "    labels = pred.label_ids\n",
        "    return {\n",
        "        \"acc\": accuracy_score(labels, predictions),\n",
        "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
        "        \"f1_weight\": f1_score(labels, predictions, average=\"weighted\")\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy0fSCSEAfO6"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=10,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_ds,              # training dataset\n",
        "    eval_dataset=eval_ds,                # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HwBTj2tXh3KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_ds,              # training dataset\n",
        "    eval_dataset=eval_ds,                # evaluation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XULVEtHgfV25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer APIs could also be used for testing the model\n",
        "preds = trainer.predict(test_ds)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "BMSGNRb8hOyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgdiLOyFDfjJ"
      },
      "source": [
        "Additional information on how to fine-tune a pre-trained model both with Trainer API or with standard PyTorch/TensorFlow (Keras) could be found [here](https://huggingface.co/docs/transformers/training).\n",
        "\n",
        "Some additional notebooks that can be useful for the project (if you want and can use HF):\n",
        "\n",
        "- PyTorch + HF: https://github.com/huggingface/transformers/tree/master/notebooks#pytorch-examples\n",
        "- TensorFlow + HF: https://github.com/huggingface/transformers/tree/master/notebooks#tensorflow-examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C15kFcl4G-zw"
      },
      "source": [
        "# Datasets & Metrics\n",
        "\n",
        "Hugging Face also provide separate packages for [datasets](https://huggingface.co/datasets) and [metrics](https://huggingface.co/metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-vDVgiyG-Tv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example on how to use a metric https://huggingface.co/docs/evaluate/choosing_a_metric\n",
        "\n",
        "from evaluate import load\n",
        "\n",
        "metric = load(\"accuracy\")"
      ],
      "metadata": {
        "id": "OBEKe9Ndh0zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = preds.predictions.argmax(-1)\n",
        "\n",
        "metric.add_batch(predictions=y_pred, references=test_y)\n",
        "\n",
        "print (metric.compute())"
      ],
      "metadata": {
        "id": "qh4VhYnOh4Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"abisee/cnn_dailymail\", '3.0.0')"
      ],
      "metadata": {
        "id": "pL99VipJh9Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0upCvokHJoXr"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"train\"][0])\n",
        "print(\"Source text:\", dataset[\"train\"][0][\"article\"])\n",
        "print(\"Target text:\", dataset[\"train\"][0][\"highlights\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrVPzMrGL8BB"
      },
      "outputs": [],
      "source": [
        "max_input_length = 512\n",
        "max_output_length = 64\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [s for s in examples[\"article\"]]\n",
        "    inputs = \" \".join(inputs)\n",
        "    targets = examples[\"highlights\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=max_output_length, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"abisee/cnn_dailymail\", '3.0.0', split='train[:1000]')\n",
        "\n",
        "dataset = dataset.map(preprocess_function)"
      ],
      "metadata": {
        "id": "h9sm0QJLpAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-C-Il3MaUr"
      },
      "outputs": [],
      "source": [
        "print(dataset[0])\n",
        "print(\"Source text:\", dataset[0][\"input_ids\"])\n",
        "print(\"Target text:\", dataset[0][\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Onc7p2N0M8bW"
      },
      "outputs": [],
      "source": [
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "dataset.set_format(type='torch', columns=columns_to_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deiOc6ckNAam"
      },
      "outputs": [],
      "source": [
        "print(dataset[0])\n",
        "print(\"Source text:\", dataset[0][\"input_ids\"])\n",
        "print(\"Target text:\", dataset[0][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's have a look at the Hugging Face ðŸ¤— website!\n",
        "\n",
        "- [Models](https://huggingface.co/models)\n",
        "- [Datasets](https://huggingface.co/models)\n",
        "- [Spaces](https://huggingface.co/spaces)\n",
        "- [Docs](https://huggingface.co/docs)"
      ],
      "metadata": {
        "id": "TVlGwbumibWh"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}