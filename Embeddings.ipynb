{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKAlWEZhpuZDRSHElLQAaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpgmina/DeepNLP/blob/main/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings: CBOW\n",
        "\n",
        "If we **stack context one-hot vectors as columns**:\n",
        "\n",
        "$$\n",
        "X = [x_1, x_2, \\dots, x_C] \\in \\mathbb{R}^{V \\times C}\n",
        "$$\n",
        "\n",
        "where $V$ is vocabulary size and $C$ is the number of context words.\n",
        "\n",
        "---\n",
        "\n",
        "**Step 1: Multiply by embedding matrix**\n",
        "\n",
        "$$\n",
        "H = E^T X \\in \\mathbb{R}^{N \\times C}\n",
        "$$\n",
        "\n",
        "- $E \\in \\mathbb{R}^{V \\times N}$ is the input embedding matrix  \n",
        "- $N$ is the embedding dimension  \n",
        "- Each column of $H$ is the embedding of a context word\n",
        "\n",
        "---\n",
        "\n",
        "**Step 2: Average columns to get hidden vector**\n",
        "\n",
        "$$\n",
        "h = \\frac{1}{C} H \\mathbf{1}_C\n",
        "$$\n",
        "\n",
        "- $\\mathbf{1}_C \\in \\mathbb{R}^{C}$ is a column vector of ones  \n",
        "- $h \\in \\mathbb{R}^{N}$ is the mean embedding of the context words\n",
        "\n",
        "---\n",
        "\n",
        "**Step 3: Compute output logits**\n",
        "\n",
        "$$\n",
        "u = U^T h \\in \\mathbb{R}^{V}\n",
        "$$\n",
        "\n",
        "- $U \\in \\mathbb{R}^{N \\times V}$ is the output weight matrix  \n",
        "\n",
        "---\n",
        "\n",
        "**Step 4: Compute softmax / loss**\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{softmax}(u)\n",
        "$$\n",
        "\n",
        "- $\\hat{y} \\in \\mathbb{R}^{V}$ is the predicted probability distribution over the vocabulary  \n",
        "- Loss is computed with **cross-entropy** against the target word\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "\n",
        "1. **Input → embedding:** $E^T X$\n",
        "2. **Aggregate context:** multiply by $\\frac{1}{C} \\mathbf{1}_C$\n",
        "3. **Hidden → vocab:** $U^T h$\n",
        "4. **Softmax:** converts logits to probability distribution\n",
        "\n",
        "> This is exactly what the CBOW model does in matrix form — fully vectorized, no loops needed.\n",
        "\n",
        "\n",
        "NB: Even though there’s **no non-linearity** (like ReLU or tanh), this setup can still *learn useful embeddings* because the model’s goal is to make words that appear in similar contexts have similar representations.  \n",
        "Adding non-linearities wouldn’t necessarily improve this — in fact, in the original **Word2Vec (CBOW)** paper, **no non-linearity** was used between the embedding and output layers.  \n",
        "The model’s simplicity is key: it’s essentially performing a form of **logistic regression** on word co-occurrence statistics."
      ],
      "metadata": {
        "id": "HKwsgIjk3fBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bxfTK6km-RF"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTINUOUS BAG OF WORDS (CBOW) MODEL\n",
        "# using nn.Linear instead of nn.Embedding\n",
        "# ------------------------------------------\n",
        "# This version is for EDUCATIONAL PURPOSES:\n",
        "# - we manually create one-hot encodings\n",
        "# - we use a linear layer to simulate embeddings\n",
        "# ==========================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 1. Define a tiny toy corpus\n",
        "# ----------------------------------------------------\n",
        "corpus = [\n",
        "    \"the quick brown fox jumped over the lazy dog\",\n",
        "    \"i love playing with my dog\",\n",
        "    \"the dog loves the fox\"\n",
        "]"
      ],
      "metadata": {
        "id": "f6buhL5CnHta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 2. Preprocess text: tokenize and build vocabulary\n",
        "# ----------------------------------------------------\n",
        "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# Flatten all tokens into a set of unique words\n",
        "vocab = sorted(set(chain.from_iterable(tokenized_corpus)))\n",
        "\n",
        "# Maps for word <-> index\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Word to index:\", word_to_ix)\n",
        "print(\"Index to word:\", ix_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkUD44oenK4M",
        "outputId": "9e2df101-5b65-4e67-9927-feb50c622820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['brown', 'dog', 'fox', 'i', 'jumped', 'lazy', 'love', 'loves', 'my', 'over', 'playing', 'quick', 'the', 'with']\n",
            "Word to index: {'brown': 0, 'dog': 1, 'fox': 2, 'i': 3, 'jumped': 4, 'lazy': 5, 'love': 6, 'loves': 7, 'my': 8, 'over': 9, 'playing': 10, 'quick': 11, 'the': 12, 'with': 13}\n",
            "Index to word: {0: 'brown', 1: 'dog', 2: 'fox', 3: 'i', 4: 'jumped', 5: 'lazy', 6: 'love', 7: 'loves', 8: 'my', 9: 'over', 10: 'playing', 11: 'quick', 12: 'the', 13: 'with'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 3. Create (context, target) pairs\n",
        "#    For each word, context = nearby words in a window\n",
        "# ----------------------------------------------------\n",
        "def make_context_windows(tokenized_corpus, window_size=2):\n",
        "    data = []\n",
        "    for sentence in tokenized_corpus:\n",
        "        for i, word in enumerate(sentence):\n",
        "            context = []\n",
        "            for j in range(-window_size, window_size + 1):\n",
        "                if j != 0 and 0 <= i + j < len(sentence):\n",
        "                    context.append(sentence[i + j])\n",
        "            if len(context) > 0:\n",
        "                data.append((context, word))\n",
        "    return data\n",
        "\n",
        "data = make_context_windows(tokenized_corpus, window_size=2)\n",
        "print(\"\\nSample (context, target) pairs:\")\n",
        "for i in range(4):\n",
        "    print(data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-j73hwMnWTm",
        "outputId": "daffb206-467a-4ee8-8e2d-65d60b247ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample (context, target) pairs:\n",
            "(['quick', 'brown'], 'the')\n",
            "(['the', 'brown', 'fox'], 'quick')\n",
            "(['the', 'quick', 'fox', 'jumped'], 'brown')\n",
            "(['quick', 'brown', 'jumped', 'over'], 'fox')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target = \"the\" (first word)\n",
        "Context = next 2 words → [\"quick\", \"brown\"] ✅\n",
        "\n",
        "Target = \"quick\" (second word)\n",
        "Context = 2 words before + 2 words after = [\"the\", \"brown\", \"fox\"] ✅\n",
        "\n",
        "Target = \"brown\" (third word)\n",
        "Context = 2 words before + 2 words after = [\"the\", \"quick\", \"fox\", \"jumped\"] ✅"
      ],
      "metadata": {
        "id": "S0mQ66HBvdR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 4. Helper: create one-hot encodings for words\n",
        "# ----------------------------------------------------\n",
        "def one_hot_vector(word_idx, vocab_size):\n",
        "    \"\"\"Return one-hot vector for a single word index.\"\"\"\n",
        "    vec = torch.zeros(vocab_size)\n",
        "    vec[word_idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "def make_tensor(context, target):\n",
        "    \"\"\"Return input (context as mean of one-hots) and target (as index).\"\"\"\n",
        "    context_vecs = torch.stack([one_hot_vector(word_to_ix[w], vocab_size) for w in context])\n",
        "    mean_context = context_vecs.mean(dim=0)   # average context representation\n",
        "    target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
        "    return mean_context.unsqueeze(0), target_idx  # shape: [1, vocab_size], [1]\n"
      ],
      "metadata": {
        "id": "cGj4KvRasQ7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 5. Define CBOW model using a linear layer\n",
        "# ----------------------------------------------------\n",
        "class CBOWLinear(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOWLinear, self).__init__()\n",
        "        # Instead of an embedding layer, we use a Linear layer\n",
        "        # The linear layer maps from vocab_size (one-hot input)\n",
        "        # -> embedding_dim (hidden representation)\n",
        "        self.linear1 = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
        "        # Then another layer projects embedding back to vocab size for prediction\n",
        "        self.linear2 = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, context_onehot):\n",
        "        \"\"\"\n",
        "        context_onehot: shape [batch_size, vocab_size]\n",
        "        (this is a bag of words averaged one-hot)\n",
        "        \"\"\"\n",
        "        hidden = self.linear1(context_onehot)  # map to embedding space\n",
        "        output = self.linear2(hidden)          # map back to vocab probabilities\n",
        "        return output"
      ],
      "metadata": {
        "id": "hSX-XP0hvwq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Should the loss (CrossEntropy) be part of the neural network?\n",
        "\n",
        "No — the **loss function** should not be part of the model class.\n",
        "\n",
        "The model’s job is to define how data flows through layers (the *forward pass*).  \n",
        "The **loss function** is a *separate component* that tells the optimizer how wrong the predictions are.\n",
        "\n",
        "Keeping the loss separate has two main benefits:\n",
        "1. ✅ It allows you to **reuse the same model** with different losses (e.g., CrossEntropy, NLLLoss, or MSE).\n",
        "2. ✅ It keeps your model definition **clean and modular**, focusing only on computation, not evaluation.\n",
        "\n",
        "So the typical workflow is:\n",
        "```python\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n"
      ],
      "metadata": {
        "id": "Ouyi2FKBvLR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### check that the architecture runs\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 10\n",
        "model = CBOWLinear(vocab_size, embedding_dim)\n",
        "# --------------------------------------------------\n",
        "# 1. Create a fake input: average of 3 one-hot vectors\n",
        "# --------------------------------------------------\n",
        "def one_hot(idx, vocab_size):\n",
        "    vec = torch.zeros(vocab_size)\n",
        "    vec[idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "# Fake context: indices [2, 5, 7]\n",
        "context_indices = [2, 5, 7]\n",
        "context_vecs = torch.stack([one_hot(i, vocab_size) for i in context_indices])\n",
        "context_input = context_vecs.mean(dim=0).unsqueeze(0)  # shape: [1, vocab_size]\n",
        "print(f\"Context input: {context_input}\")\n",
        "# --------------------------------------------------\n",
        "# 2. Forward pass\n",
        "# --------------------------------------------------\n",
        "output = model(context_input)\n",
        "print(\"Output shape:\", output.shape)\n",
        "# Expected: [1, vocab_size] → raw logits for each word\n",
        "print(\"Output logits:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMhHJ86WwcZf",
        "outputId": "90384cb0-8cef-4bb1-d691-26443b09f97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context input: tensor([[0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
            "Output shape: torch.Size([1, 14])\n",
            "Output logits: tensor([[-0.0067,  0.0452,  0.0626, -0.0505,  0.0582, -0.0175,  0.0010, -0.0029,\n",
            "          0.1141,  0.0109,  0.0526,  0.0113, -0.0143,  0.0754]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 6. Initialize model, loss, and optimizer\n",
        "# ----------------------------------------------------\n",
        "embedding_dim = 10\n",
        "model = CBOWLinear(vocab_size, embedding_dim)\n",
        "\n",
        "# Cross-entropy is appropriate for classification\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7. Training loop\n",
        "# ----------------------------------------------------\n",
        "print(\"\\nTraining CBOW with nn.Linear ...\")\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        # Prepare input tensors\n",
        "        context_tensor, target_tensor = make_tensor(context, target)\n",
        "\n",
        "        # Forward pass\n",
        "        pred = model(context_tensor)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(pred, target_tensor)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:3d} | Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REmu-7DKw3m8",
        "outputId": "3edbc1f6-18d8-4b7d-9c3e-a5d8ddc891b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training CBOW with nn.Linear ...\n",
            "Epoch   0 | Loss: 52.8511\n",
            "Epoch  10 | Loss: 26.3736\n",
            "Epoch  20 | Loss: 10.6341\n",
            "Epoch  30 | Loss: 4.3213\n",
            "Epoch  40 | Loss: 2.0580\n",
            "Epoch  50 | Loss: 1.1233\n",
            "Epoch  60 | Loss: 0.6902\n",
            "Epoch  70 | Loss: 0.4621\n",
            "Epoch  80 | Loss: 0.3285\n",
            "Epoch  90 | Loss: 0.2437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------\n",
        "# 8. Inspect learned embeddings\n",
        "# ----------------------------------------------------\n",
        "# The \"embedding\" for each word is just the corresponding row\n",
        "# of the first linear layer's weight matrix\n",
        "with torch.no_grad():\n",
        "    embedding_matrix = model.linear1.weight.data  # shape [embedding_dim, vocab_size]\n",
        "    # Transpose it so each row corresponds to a word\n",
        "    embedding_matrix = embedding_matrix.T         # [vocab_size, embedding_dim]\n",
        "\n",
        "# Show a few word vectors\n",
        "print(\"\\nSample word embeddings (rows correspond to words):\")\n",
        "for i, word in enumerate(vocab[:5]):\n",
        "    print(f\"{word:>10s} : {embedding_matrix[i].numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7nU9NXIxUrS",
        "outputId": "b90204db-b2d4-4f8d-fa76-675a388d3127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample word embeddings (rows correspond to words):\n",
            "     brown : [-0.77698165 -1.2045684  -0.2711266  -2.0864468   0.05063495 -0.21035163\n",
            "  1.1458198  -2.808613   -0.69390845 -1.6719401 ]\n",
            "       dog : [ 1.7378042   0.81062865 -3.9599147   2.3803916   1.0353929  -0.3023908\n",
            " -1.1362057  -1.5668218  -2.71045    -2.68095   ]\n",
            "       fox : [-0.5113443  -1.1659439  -3.1197329  -1.6425169  -1.3973596  -0.04413448\n",
            " -1.0791274  -0.85332245  1.0101377  -1.8844566 ]\n",
            "         i : [-2.1445022  -0.28397575  2.3767862   1.6141582   2.061398    1.2741618\n",
            " -1.9728276   2.297368    0.23704498  0.7643219 ]\n",
            "    jumped : [ 2.9816546  -0.2211122   1.4346014  -3.3715289   0.20837057  3.0859766\n",
            " -0.9067423   1.3982038   1.748613   -0.29846627]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 9. Compute word similarity using cosine similarity\n",
        "# ----------------------------------------------------\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return torch.dot(vec1, vec2) / (vec1.norm() * vec2.norm())\n",
        "\n",
        "def most_similar(word, topn=5):\n",
        "    idx = word_to_ix[word]\n",
        "    word_vec = embedding_matrix[idx]\n",
        "    sims = []\n",
        "    for other_word, j in word_to_ix.items():\n",
        "        if other_word != word:\n",
        "            sim = cosine_similarity(word_vec, embedding_matrix[j])\n",
        "            sims.append((other_word, sim.item()))\n",
        "    sims.sort(key=lambda x: -x[1])\n",
        "    return sims[:topn]\n",
        "\n",
        "print(\"\\nMost similar words to 'dog':\")\n",
        "for w, s in most_similar(\"dog\"):\n",
        "    print(f\"{w:>10s} : {s:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LuAAe5SnFRJ",
        "outputId": "bb824b49-d0b7-4f5d-9884-cd07f6831c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar words to 'dog':\n",
            "     quick : 0.407\n",
            "       fox : 0.318\n",
            "      lazy : 0.133\n",
            "     brown : 0.116\n",
            "        my : 0.042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVBsV4bZxXIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}